{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install onnx onnxruntime opencv-python ultralytics pyyaml matplotlib\n",
        "!pip install --upgrade protobuf  # Upgrade protobuf as it can cause issues with ONNX\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XyGonTDGoJR",
        "outputId": "c0d86963-ca71-4df9-89ea-9bf99043fad2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.138-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics-8.3.138-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.18.0 onnxruntime-1.22.0 ultralytics-8.3.138 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Downloading protobuf-6.31.0-cp39-abi3-manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.9/320.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 6.31.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 6.31.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-6.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Enhanced YOLOv12n ONNX Export and INT8 Quantization for Google Colab\n",
        "====================================================================\n",
        "This script handles:\n",
        "1. Loading the enhanced YOLOv12n model from Google Drive\n",
        "2. Exporting to ONNX with custom modules preserved\n",
        "3. Applying INT8 quantization with calibration or direct quantization\n",
        "4. Verifying the quantized model's accuracy\n",
        "5. Optimizing for deployment on bodycam hardware\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType, QuantFormat\n",
        "from onnxruntime.quantization.calibrate import CalibrationMethod\n",
        "from onnxruntime.quantization.quantize import quantize_dynamic\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "import json\n",
        "import time\n",
        "import yaml\n",
        "from typing import List, Dict, Tuple, Union, Any, Optional\n",
        "from google.colab import drive\n",
        "\n",
        "# =====================================\n",
        "# PART 1: CONFIGURATION\n",
        "# =====================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration for export and quantization\"\"\"\n",
        "\n",
        "    # Mount Google Drive paths\n",
        "    MOUNT_POINT = '/content/drive'\n",
        "\n",
        "    # Model paths\n",
        "    MODEL_PT_PATH = '/content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/enhanced_yolov12n/weights/best.pt'\n",
        "    DATA_YAML_PATH = '/content/drive/MyDrive/SemesterProjectDatas/CombinedData/data.yaml'\n",
        "    ONNX_OUTPUT_DIR = '/content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized'\n",
        "    ONNX_MODEL_PATH = os.path.join(ONNX_OUTPUT_DIR, \"yolov12n_enhanced.onnx\")\n",
        "    ONNX_INT8_MODEL_PATH = os.path.join(ONNX_OUTPUT_DIR, \"yolov12n_enhanced_int8.onnx\")\n",
        "\n",
        "    # Calibration settings - will be determined during setup\n",
        "    CALIBRATION_DATA_DIR = None  # Will be set during setup\n",
        "    NUM_CALIBRATION_IMAGES = 100\n",
        "\n",
        "    # Export settings\n",
        "    BATCH_SIZE = 1\n",
        "    INPUT_SIZE = (640, 640)\n",
        "    OPSET_VERSION = 14\n",
        "\n",
        "    # Quantization settings\n",
        "    QUANT_FORMAT = QuantFormat.QOperator  # Changed from QDQ to QOperator for better compatibility\n",
        "    WEIGHT_TYPE = QuantType.QInt8\n",
        "    ACTIVATION_TYPE = QuantType.QUInt8\n",
        "    CALIBRATION_METHOD = CalibrationMethod.MinMax  # Changed from Entropy to MinMax for better compatibility\n",
        "    PERCENTILE = 99.99  # Only used if calibration method is Percentile\n",
        "    PER_CHANNEL = True\n",
        "\n",
        "    # Evaluation settings - will be determined during setup\n",
        "    EVAL_DATA_DIR = None  # Will be set during setup\n",
        "    CONFIDENCE_THRESHOLD = 0.25\n",
        "    IOU_THRESHOLD = 0.45\n",
        "\n",
        "    # Hardware optimization\n",
        "    NUM_THREADS = 2  # Number of CPU threads for inference\n",
        "\n",
        "    @classmethod\n",
        "    def find_directories_with_images(cls, base_dir, max_depth=4):\n",
        "        \"\"\"Find directories containing images recursively up to max_depth\"\"\"\n",
        "        image_dirs = []\n",
        "\n",
        "        def _explore_dir(current_dir, depth):\n",
        "            if depth > max_depth:\n",
        "                return\n",
        "\n",
        "            try:\n",
        "                # Check if current dir has images\n",
        "                has_images = False\n",
        "                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
        "                    if glob(os.path.join(current_dir, ext)):\n",
        "                        has_images = True\n",
        "                        break\n",
        "\n",
        "                if has_images:\n",
        "                    image_dirs.append(current_dir)\n",
        "\n",
        "                # Explore subdirectories\n",
        "                for item in os.listdir(current_dir):\n",
        "                    item_path = os.path.join(current_dir, item)\n",
        "                    if os.path.isdir(item_path):\n",
        "                        _explore_dir(item_path, depth + 1)\n",
        "            except Exception as e:\n",
        "                print(f\"Error exploring {current_dir}: {e}\")\n",
        "\n",
        "        # Start exploration\n",
        "        _explore_dir(base_dir, 0)\n",
        "        return image_dirs\n",
        "\n",
        "    @classmethod\n",
        "    def setup(cls):\n",
        "        \"\"\"Create necessary directories and mount drive\"\"\"\n",
        "        # Mount Google Drive if needed\n",
        "        if not os.path.exists(cls.MOUNT_POINT):\n",
        "            print(\"Mounting Google Drive...\")\n",
        "            drive.mount(cls.MOUNT_POINT)\n",
        "\n",
        "        print(\"Verifying Google Drive mount...\")\n",
        "        if not os.path.exists(cls.MOUNT_POINT):\n",
        "            raise RuntimeError(\"Google Drive mount failed. Cannot proceed.\")\n",
        "        else:\n",
        "            print(\"✓ Google Drive mounted successfully\")\n",
        "\n",
        "        # Verify project directory structure\n",
        "        project_base = '/content/drive/My Drive/SemesterProjectDatas'\n",
        "        if not os.path.exists(project_base):\n",
        "            project_base = '/content/drive/MyDrive/SemesterProjectDatas'\n",
        "            if not os.path.exists(project_base):\n",
        "                print(\"⚠ Warning: Project base directory not found at either path.\")\n",
        "                print(\"Available directories in Google Drive:\")\n",
        "                for item in os.listdir('/content/drive'):\n",
        "                    print(f\"  - /content/drive/{item}\")\n",
        "\n",
        "                print(\"\\nPlease enter the correct path to your SemesterProjectDatas folder:\")\n",
        "                user_path = input().strip()\n",
        "                if os.path.exists(user_path):\n",
        "                    project_base = user_path\n",
        "                    print(f\"✓ Using user-provided path: {project_base}\")\n",
        "                else:\n",
        "                    print(f\"⚠ Path not found: {user_path}\")\n",
        "                    print(\"Continuing with default path, but this may cause errors\")\n",
        "\n",
        "        print(f\"Project base directory: {project_base}\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(cls.ONNX_OUTPUT_DIR, exist_ok=True)\n",
        "        print(f\"Output directory created/verified: {cls.ONNX_OUTPUT_DIR}\")\n",
        "\n",
        "        # Check model path\n",
        "        if not os.path.exists(cls.MODEL_PT_PATH):\n",
        "            print(f\"⚠ Warning: Model not found at {cls.MODEL_PT_PATH}\")\n",
        "            print(\"Searching for model files in project directory...\")\n",
        "\n",
        "            # Look for PT files recursively\n",
        "            pt_files = []\n",
        "            for root, dirs, files in os.walk(project_base):\n",
        "                for file in files:\n",
        "                    if file.endswith('.pt') and 'best' in file.lower():\n",
        "                        pt_files.append(os.path.join(root, file))\n",
        "\n",
        "            if pt_files:\n",
        "                print(\"Found model candidates:\")\n",
        "                for i, path in enumerate(pt_files):\n",
        "                    print(f\"  [{i}] {path}\")\n",
        "\n",
        "                print(\"\\nEnter the number of the correct model (or press Enter to use the first one):\")\n",
        "                user_choice = input().strip()\n",
        "\n",
        "                try:\n",
        "                    idx = int(user_choice) if user_choice else 0\n",
        "                    cls.MODEL_PT_PATH = pt_files[idx]\n",
        "                    print(f\"✓ Selected model: {cls.MODEL_PT_PATH}\")\n",
        "                except (ValueError, IndexError):\n",
        "                    if pt_files:\n",
        "                        cls.MODEL_PT_PATH = pt_files[0]\n",
        "                        print(f\"✓ Using first model: {cls.MODEL_PT_PATH}\")\n",
        "\n",
        "        # Find image directories for calibration and evaluation\n",
        "        print(\"\\nSearching for image directories in the project...\")\n",
        "        possible_dirs = cls.find_directories_with_images(project_base)\n",
        "\n",
        "        if possible_dirs:\n",
        "            print(\"\\nFound directories with images:\")\n",
        "            for i, path in enumerate(possible_dirs):\n",
        "                num_images = len(glob(os.path.join(path, '*.jpg'))) + len(glob(os.path.join(path, '*.png')))\n",
        "                print(f\"  [{i}] {path} ({num_images} images)\")\n",
        "\n",
        "            print(\"\\nEnter the number of the directory to use for CALIBRATION:\")\n",
        "            user_choice = input().strip()\n",
        "\n",
        "            try:\n",
        "                idx = int(user_choice) if user_choice else 0\n",
        "                cls.CALIBRATION_DATA_DIR = possible_dirs[idx]\n",
        "                print(f\"✓ Selected calibration directory: {cls.CALIBRATION_DATA_DIR}\")\n",
        "            except (ValueError, IndexError):\n",
        "                if possible_dirs:\n",
        "                    cls.CALIBRATION_DATA_DIR = possible_dirs[0]\n",
        "                    print(f\"✓ Using first directory for calibration: {cls.CALIBRATION_DATA_DIR}\")\n",
        "\n",
        "            print(\"\\nEnter the number of the directory to use for EVALUATION (or press Enter to use the same as calibration):\")\n",
        "            user_choice = input().strip()\n",
        "\n",
        "            try:\n",
        "                if user_choice:\n",
        "                    idx = int(user_choice)\n",
        "                    cls.EVAL_DATA_DIR = possible_dirs[idx]\n",
        "                else:\n",
        "                    cls.EVAL_DATA_DIR = cls.CALIBRATION_DATA_DIR\n",
        "                print(f\"✓ Selected evaluation directory: {cls.EVAL_DATA_DIR}\")\n",
        "            except (ValueError, IndexError):\n",
        "                cls.EVAL_DATA_DIR = cls.CALIBRATION_DATA_DIR\n",
        "                print(f\"✓ Using same directory for evaluation: {cls.EVAL_DATA_DIR}\")\n",
        "        else:\n",
        "            print(\"⚠ No image directories found in the project.\")\n",
        "            print(\"Please manually specify paths...\")\n",
        "\n",
        "            print(\"\\nEnter the full path to your calibration images directory:\")\n",
        "            cls.CALIBRATION_DATA_DIR = input().strip()\n",
        "\n",
        "            print(\"\\nEnter the full path to your evaluation images directory (or press Enter to use the same as calibration):\")\n",
        "            eval_dir = input().strip()\n",
        "            cls.EVAL_DATA_DIR = eval_dir if eval_dir else cls.CALIBRATION_DATA_DIR\n",
        "\n",
        "        # Load class names from data.yaml\n",
        "        try:\n",
        "            with open(cls.DATA_YAML_PATH, 'r') as f:\n",
        "                cls.data_dict = yaml.safe_load(f)\n",
        "            print(f\"Loaded data config with {len(cls.data_dict['names'])} classes\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load class names from {cls.DATA_YAML_PATH}: {e}\")\n",
        "            cls.data_dict = {'names': [f\"class_{i}\" for i in range(10)]}  # Fallback\n",
        "\n",
        "# =====================================\n",
        "# PART 2: MODEL PREPARATION AND EXPORT\n",
        "# =====================================\n",
        "\n",
        "def prepare_model(model_path: str) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Load the YOLOv12n model and prepare it for export\n",
        "    Args:\n",
        "        model_path: Path to the trained PyTorch model\n",
        "    Returns:\n",
        "        PyTorch model ready for export\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the YOLO model using ultralytics\n",
        "        model = YOLO(model_path)\n",
        "        print(\"✓ Model loaded successfully using ultralytics YOLO\")\n",
        "\n",
        "        # Set model to evaluation mode\n",
        "        model.model.eval()\n",
        "\n",
        "        # Verify enhancement modules are present\n",
        "        check_enhancement_modules(model.model)\n",
        "\n",
        "        # Use the PyTorch model directly\n",
        "        return model.model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error loading model with ultralytics: {e}\")\n",
        "\n",
        "        # Fallback to PyTorch loading\n",
        "        try:\n",
        "            print(\"Trying to load model directly with PyTorch...\")\n",
        "            model = torch.load(model_path, map_location='cpu')\n",
        "\n",
        "            # Check if we got the state_dict instead of the model\n",
        "            if isinstance(model, dict) and 'model' in model:\n",
        "                print(\"Loaded state_dict, extracting model...\")\n",
        "                model = model['model']\n",
        "\n",
        "            model.eval()\n",
        "            print(\"✓ Model loaded successfully using PyTorch\")\n",
        "\n",
        "            # Check for enhancement modules\n",
        "            check_enhancement_modules(model)\n",
        "\n",
        "            return model\n",
        "        except Exception as e2:\n",
        "            print(f\"⚠ Error loading model with PyTorch: {e2}\")\n",
        "            raise RuntimeError(f\"Failed to load model using both methods: {e}, {e2}\")\n",
        "\n",
        "def check_enhancement_modules(model: nn.Module) -> None:\n",
        "    \"\"\"\n",
        "    Verify that enhancement modules exist in the model\n",
        "    Args:\n",
        "        model: PyTorch model to check\n",
        "    \"\"\"\n",
        "    has_cbam = False\n",
        "    has_transformer = False\n",
        "    has_sof = False\n",
        "    has_bifpn = False\n",
        "\n",
        "    # Check for our custom modules by recursively inspecting model layers\n",
        "    for name, module in model.named_modules():\n",
        "        class_name = module.__class__.__name__\n",
        "        if 'CBAM' in class_name:\n",
        "            has_cbam = True\n",
        "        elif 'TransformerEncoder' in class_name:\n",
        "            has_transformer = True\n",
        "        elif 'SmallObjectFeatures' in class_name:\n",
        "            has_sof = True\n",
        "        elif 'BiFPN' in class_name:\n",
        "            has_bifpn = True\n",
        "\n",
        "    # Report findings\n",
        "    print(\"Enhancement modules detection:\")\n",
        "    print(f\"  - CBAM: {'Found' if has_cbam else 'Not found'}\")\n",
        "    print(f\"  - Transformer: {'Found' if has_transformer else 'Not found'}\")\n",
        "    print(f\"  - SmallObjectFeatures: {'Found' if has_sof else 'Not found'}\")\n",
        "    print(f\"  - BiFPN: {'Found' if has_bifpn else 'Not found'}\")\n",
        "\n",
        "    # Warn if any expected modules are missing\n",
        "    if not all([has_cbam, has_transformer, has_sof, has_bifpn]):\n",
        "        print(\"WARNING: Some enhancement modules were not detected in the model.\")\n",
        "\n",
        "def export_to_onnx(model: nn.Module, onnx_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Export PyTorch model to ONNX format\n",
        "    Args:\n",
        "        model: PyTorch model to export\n",
        "        onnx_path: Output path for ONNX model\n",
        "    \"\"\"\n",
        "    print(f\"Exporting model to ONNX: {onnx_path}\")\n",
        "\n",
        "    # Create dummy input tensor\n",
        "    dummy_input = torch.randn(\n",
        "        Config.BATCH_SIZE,\n",
        "        3,\n",
        "        Config.INPUT_SIZE[0],\n",
        "        Config.INPUT_SIZE[1]\n",
        "    ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Get input and output names from the model\n",
        "    input_names = [\"input\"]\n",
        "    output_names = [\"output\"]\n",
        "\n",
        "    # Dynamic axes for variable batch size and image dimensions\n",
        "    dynamic_axes = {\n",
        "        \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},\n",
        "        \"output\": {0: \"batch_size\"}\n",
        "    }\n",
        "\n",
        "    # First, ensure the model is in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # ONNX export options to handle the custom attention modules better\n",
        "    export_options = {\n",
        "        \"verbose\": False,\n",
        "        \"export_params\": True,\n",
        "        \"opset_version\": Config.OPSET_VERSION,\n",
        "        \"do_constant_folding\": True,\n",
        "        \"input_names\": input_names,\n",
        "        \"output_names\": output_names,\n",
        "        \"dynamic_axes\": dynamic_axes,\n",
        "    }\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        # Try exporting with standard settings\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            dummy_input,\n",
        "            onnx_path,\n",
        "            **export_options\n",
        "        )\n",
        "        print(\"✓ ONNX export completed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Standard ONNX export failed: {e}\")\n",
        "        print(\"Trying with additional settings for custom modules...\")\n",
        "\n",
        "        # Add more options to handle custom modules\n",
        "        export_options.update({\n",
        "            \"keep_initializers_as_inputs\": True,\n",
        "            \"enable_onnx_checker\": False,  # Disable strict checking to allow custom ops\n",
        "        })\n",
        "\n",
        "        try:\n",
        "            torch.onnx.export(\n",
        "                model,\n",
        "                dummy_input,\n",
        "                onnx_path,\n",
        "                **export_options\n",
        "            )\n",
        "            print(\"✓ ONNX export with relaxed settings completed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ ONNX export failed: {e}\")\n",
        "            print(\"Trying with TraceError handling...\")\n",
        "\n",
        "            # Try with explicit loop tracing using torch.jit\n",
        "            try:\n",
        "                # Create traced model first\n",
        "                with torch.no_grad():\n",
        "                    traced_model = torch.jit.trace(model, dummy_input)\n",
        "\n",
        "                # Export the traced model\n",
        "                torch.onnx.export(\n",
        "                    traced_model,\n",
        "                    dummy_input,\n",
        "                    onnx_path,\n",
        "                    **export_options\n",
        "                )\n",
        "                print(\"✓ ONNX export with JIT tracing completed successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ All ONNX export methods failed: {e}\")\n",
        "                print(\"Please simplify your model or check for custom operations\")\n",
        "                raise RuntimeError(\"ONNX export failed after multiple attempts\")\n",
        "\n",
        "    # Verify the exported model\n",
        "    try:\n",
        "        onnx_model = onnx.load(onnx_path)\n",
        "        onnx.checker.check_model(onnx_model)\n",
        "        print(\"✓ ONNX model structure verified\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ ONNX model verification warning: {e}\")\n",
        "        print(\"The model may still work despite verification warnings\")\n",
        "\n",
        "def verify_onnx_model(onnx_path: str, model: nn.Module) -> None:\n",
        "    \"\"\"\n",
        "    Verify that the ONNX model produces the same outputs as the PyTorch model\n",
        "    Args:\n",
        "        onnx_path: Path to the exported ONNX model\n",
        "        model: Original PyTorch model\n",
        "    \"\"\"\n",
        "    print(\"Verifying ONNX model...\")\n",
        "\n",
        "    # First, check ONNX model validity\n",
        "    onnx_model = onnx.load(onnx_path)\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(\"✓ ONNX model structure is valid\")\n",
        "\n",
        "    # Create dummy input\n",
        "    dummy_input = torch.randn(\n",
        "        1,  # Use batch size 1 for verification\n",
        "        3,\n",
        "        Config.INPUT_SIZE[0],\n",
        "        Config.INPUT_SIZE[1]\n",
        "    ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Get PyTorch output\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            torch_output = model(dummy_input)\n",
        "\n",
        "            # Handle case where model returns a tuple/list\n",
        "            if isinstance(torch_output, (tuple, list)):\n",
        "                torch_output = torch_output[0]\n",
        "\n",
        "            torch_output = torch_output.cpu().numpy()\n",
        "\n",
        "            # Get ONNX Runtime output\n",
        "            ort_session = ort.InferenceSession(onnx_path)\n",
        "            input_name = ort_session.get_inputs()[0].name\n",
        "            ort_inputs = {input_name: dummy_input.cpu().numpy()}\n",
        "            ort_outputs = ort_session.run(None, ort_inputs)[0]\n",
        "\n",
        "            # Compare outputs\n",
        "            try:\n",
        "                np.testing.assert_allclose(torch_output, ort_outputs, rtol=1e-03, atol=1e-05)\n",
        "                print(\"✓ ONNX model verification passed - outputs match within tolerance\")\n",
        "            except AssertionError as e:\n",
        "                print(\"⚠ ONNX model verification partial - outputs have some differences\")\n",
        "\n",
        "                # Calculate and print error statistics for debugging\n",
        "                abs_diff = np.abs(torch_output - ort_outputs)\n",
        "                print(f\"Max absolute difference: {np.max(abs_diff)}\")\n",
        "                print(f\"Mean absolute difference: {np.mean(abs_diff)}\")\n",
        "                print(f\"Median absolute difference: {np.median(abs_diff)}\")\n",
        "\n",
        "                # Check if differences are small enough to proceed\n",
        "                if np.mean(abs_diff) < 0.1:\n",
        "                    print(\"Differences are likely due to numerical precision and acceptable\")\n",
        "                else:\n",
        "                    print(\"⚠ Large differences detected, but continuing with caution\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ ONNX model verification error: {e}\")\n",
        "            print(\"This may be due to custom operations or the complex structure of enhanced YOLO\")\n",
        "            print(\"Continuing with caution - the model may still work correctly despite verification issues\")\n",
        "\n",
        "# =====================================\n",
        "# PART 3: QUANTIZATION IMPLEMENTATION\n",
        "# =====================================\n",
        "\n",
        "class YOLOCalibrationDataReader(CalibrationDataReader):\n",
        "    \"\"\"\n",
        "    Calibration data reader for YOLO models\n",
        "    Reads and preprocesses calibration images for quantization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_folder: str,\n",
        "        input_name: str = \"input\",\n",
        "        size: Tuple[int, int] = (640, 640),\n",
        "        num_images: int = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the calibration data reader\n",
        "        Args:\n",
        "            image_folder: Path to folder containing calibration images\n",
        "            input_name: Name of the input tensor in the ONNX model\n",
        "            size: Image size (height, width) for preprocessing\n",
        "            num_images: Maximum number of images to use (None for all)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.image_folder = image_folder\n",
        "        self.input_name = input_name\n",
        "        self.size = size\n",
        "\n",
        "        # Check if folder exists\n",
        "        if not os.path.exists(image_folder):\n",
        "            raise ValueError(f\"Calibration folder does not exist: {image_folder}\")\n",
        "\n",
        "        # Get all image files in the folder\n",
        "        self.image_list = []\n",
        "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
        "            self.image_list.extend(glob(os.path.join(image_folder, ext)))\n",
        "\n",
        "        # Print first few images for debugging\n",
        "        print(f\"First few calibration images (up to 5):\")\n",
        "        for i, img_path in enumerate(self.image_list[:5]):\n",
        "            print(f\"  {i+1}. {img_path}\")\n",
        "\n",
        "        if not self.image_list:\n",
        "            raise ValueError(f\"No images found in calibration folder: {image_folder}\")\n",
        "\n",
        "        # Limit number of images if specified\n",
        "        if num_images is not None and num_images < len(self.image_list):\n",
        "            self.image_list = self.image_list[:num_images]\n",
        "\n",
        "        print(f\"Found {len(self.image_list)} calibration images in {image_folder}\")\n",
        "\n",
        "        # Verify we can actually load at least one image\n",
        "        try:\n",
        "            test_img = cv2.imread(self.image_list[0])\n",
        "            if test_img is None:\n",
        "                raise ValueError(f\"Failed to load test image: {self.image_list[0]}\")\n",
        "            print(f\"✓ Successfully loaded test image: shape={test_img.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Warning: Failed to load test image: {e}\")\n",
        "\n",
        "        self.current_idx = 0\n",
        "        self.yielded_images = 0\n",
        "\n",
        "    def get_next(self) -> Optional[Dict[str, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Get the next calibration image\n",
        "        Returns:\n",
        "            Dictionary mapping input name to preprocessed image tensor,\n",
        "            or None if all images have been processed\n",
        "        \"\"\"\n",
        "        if self.current_idx >= len(self.image_list):\n",
        "            print(f\"Calibration complete. Processed {self.yielded_images} images.\")\n",
        "            return None\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img_path = self.image_list[self.current_idx]\n",
        "        try:\n",
        "            img = cv2.imread(img_path)\n",
        "\n",
        "            # Check if image was loaded correctly\n",
        "            if img is None:\n",
        "                print(f\"Warning: cv2.imread failed for {img_path}, trying alternative loading method\")\n",
        "                try:\n",
        "                    # Try using PIL instead\n",
        "                    from PIL import Image\n",
        "                    pil_img = Image.open(img_path)\n",
        "                    img = np.array(pil_img)\n",
        "                    if pil_img.mode == 'RGB':\n",
        "                        # Convert RGB to BGR for cv2 compatibility\n",
        "                        img = img[:, :, ::-1].copy()\n",
        "                except Exception as e:\n",
        "                    print(f\"Alternative loading method also failed: {e}\")\n",
        "                    # Skip this image\n",
        "                    self.current_idx += 1\n",
        "                    return self.get_next()\n",
        "\n",
        "            # Resize and normalize\n",
        "            img = cv2.resize(img, self.size)\n",
        "            img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "            img = img.transpose(2, 0, 1)  # HWC to CHW\n",
        "            img = np.expand_dims(img, 0)  # Add batch dimension\n",
        "\n",
        "            self.current_idx += 1\n",
        "            self.yielded_images += 1\n",
        "\n",
        "            # Print progress occasionally\n",
        "            if self.yielded_images % 10 == 0:\n",
        "                print(f\"Calibration progress: {self.yielded_images}/{len(self.image_list)} images\")\n",
        "\n",
        "            return {self.input_name: img}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_path}: {e}\")\n",
        "            self.current_idx += 1\n",
        "            return self.get_next()  # Skip problematic images\n",
        "\n",
        "\n",
        "def identify_attention_nodes(onnx_model_path: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identify nodes related to attention mechanisms that should be excluded from quantization\n",
        "    Args:\n",
        "        onnx_model_path: Path to the ONNX model\n",
        "    Returns:\n",
        "        List of node names to exclude from quantization\n",
        "    \"\"\"\n",
        "    print(\"Identifying attention-related nodes to protect during quantization...\")\n",
        "\n",
        "    # Load the ONNX model\n",
        "    model_proto = onnx.load(onnx_model_path)\n",
        "\n",
        "    # Keywords that might indicate attention-related nodes\n",
        "    attention_keywords = [\n",
        "        'cbam', 'attention', 'channel_att', 'spatial_att',\n",
        "        'self_attn', 'mha', 'multihead', 'transformer',\n",
        "        'softmax', 'sigmoid', 'layernorm', 'normalization'\n",
        "    ]\n",
        "\n",
        "    # Initialize list of nodes to exclude from quantization\n",
        "    nodes_to_exclude = []\n",
        "\n",
        "    # Iterate through all nodes in the graph\n",
        "    for node in model_proto.graph.node:\n",
        "        # Check if node name contains any attention-related keywords\n",
        "        if any(kw in node.name.lower() for kw in attention_keywords):\n",
        "            nodes_to_exclude.append(node.name)\n",
        "\n",
        "        # For softmax/sigmoid operations in particular\n",
        "        if node.op_type in ['Softmax', 'Sigmoid']:\n",
        "            nodes_to_exclude.append(node.name)\n",
        "\n",
        "    print(f\"Found {len(nodes_to_exclude)} attention-related nodes to protect\")\n",
        "    return nodes_to_exclude\n",
        "\n",
        "\n",
        "def direct_quantization(onnx_model_path, output_path, nodes_to_exclude=None):\n",
        "    \"\"\"\n",
        "    Perform direct quantization without using a calibration dataset\n",
        "    Args:\n",
        "        onnx_model_path: Path to the input ONNX model\n",
        "        output_path: Path for the quantized output model\n",
        "        nodes_to_exclude: List of nodes to exclude from quantization\n",
        "    \"\"\"\n",
        "    print(\"Performing direct INT8 quantization without calibration...\")\n",
        "\n",
        "    # Check ONNX Runtime version first\n",
        "    print(f\"ONNX Runtime version: {ort.__version__}\")\n",
        "\n",
        "    # Determine which op types to quantize\n",
        "    op_types_to_quantize = [\n",
        "        'Conv', 'MatMul', 'Gemm', 'Add', 'Mul',\n",
        "        'Concat', 'MaxPool', 'AveragePool', 'Resize'\n",
        "    ]\n",
        "\n",
        "    # First try with QOperator format, which has better compatibility\n",
        "    try:\n",
        "        print(\"Trying QOperator format quantization (better compatibility)...\")\n",
        "        from onnxruntime.quantization.quantize import quantize_dynamic\n",
        "\n",
        "        quantize_dynamic(\n",
        "            model_input=onnx_model_path,\n",
        "            model_output=output_path,\n",
        "            weight_type=Config.WEIGHT_TYPE,  # QInt8\n",
        "            op_types_to_quantize=op_types_to_quantize,\n",
        "            nodes_to_exclude=nodes_to_exclude,\n",
        "            use_external_data_format=False\n",
        "        )\n",
        "        print(f\"✓ Dynamic quantization with QOperator format completed successfully: {output_path}\")\n",
        "\n",
        "        # Verify model can be loaded\n",
        "        try:\n",
        "            test_session = ort.InferenceSession(output_path)\n",
        "            print(\"✓ Quantized model verified to load correctly\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Quantized model verification failed: {e}\")\n",
        "            print(\"Will try alternative quantization approach...\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Dynamic quantization with QOperator format failed: {e}\")\n",
        "\n",
        "    # Try TensorRT conversion if on CUDA\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            print(\"GPU detected, trying TensorRT optimization...\")\n",
        "            import tensorrt as trt\n",
        "            import pycuda.driver as cuda\n",
        "            import pycuda.autoinit\n",
        "\n",
        "            # Follow TensorRT conversion path here...\n",
        "            print(\"TensorRT conversion not implemented yet in this script\")\n",
        "        except ImportError:\n",
        "            print(\"TensorRT not available, skipping this optimization\")\n",
        "\n",
        "    # Try model optimization with ONNX Runtime\n",
        "    try:\n",
        "        print(\"Trying ONNX Runtime model optimization without quantization...\")\n",
        "        from onnxruntime.transformers import optimizer\n",
        "\n",
        "        # Create directory for optimized model\n",
        "        optimized_model_path = output_path.replace('.onnx', '_optimized.onnx')\n",
        "\n",
        "        # Configure session options for optimization\n",
        "        sess_options = ort.SessionOptions()\n",
        "        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        sess_options.optimized_model_filepath = optimized_model_path\n",
        "\n",
        "        # Create session to optimize the model\n",
        "        _ = ort.InferenceSession(onnx_model_path, sess_options)\n",
        "\n",
        "        if os.path.exists(optimized_model_path):\n",
        "            print(f\"✓ Model optimization completed successfully: {optimized_model_path}\")\n",
        "            # Copy optimized model to intended output path\n",
        "            import shutil\n",
        "            shutil.copy(optimized_model_path, output_path)\n",
        "            print(f\"✓ Optimized model copied to: {output_path}\")\n",
        "\n",
        "            # Verify model can be loaded\n",
        "            try:\n",
        "                test_session = ort.InferenceSession(output_path)\n",
        "                print(\"✓ Optimized model verified to load correctly\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Optimized model verification failed: {e}\")\n",
        "        else:\n",
        "            print(f\"⚠ Model optimization failed, output file not found\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ ONNX Runtime optimization failed: {e}\")\n",
        "\n",
        "    # If all else fails, just copy the original model\n",
        "    try:\n",
        "        print(\"Falling back to using original model without quantization\")\n",
        "        import shutil\n",
        "        shutil.copy(onnx_model_path, output_path)\n",
        "        print(f\"✓ Copied original model to: {output_path}\")\n",
        "\n",
        "        # Try to optimize model using lower level API\n",
        "        with open(output_path, 'rb') as model_file:\n",
        "            model_bytes = model_file.read()\n",
        "\n",
        "        # Configure session options for optimization\n",
        "        sess_options = ort.SessionOptions()\n",
        "        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        sess_options.intra_op_num_threads = Config.NUM_THREADS\n",
        "        sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
        "        sess_options.enable_mem_pattern = True\n",
        "        sess_options.enable_mem_reuse = True\n",
        "\n",
        "        # Simply loading the model with these options may optimize it\n",
        "        _ = ort.InferenceSession(model_bytes, sess_options)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ All quantization methods failed: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def quantize_onnx_model(\n",
        "    onnx_model_path: str,\n",
        "    output_path: str,\n",
        "    calibration_data_reader: CalibrationDataReader,\n",
        "    nodes_to_exclude: List[str] = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Quantize the ONNX model to INT8 with calibration\n",
        "    Args:\n",
        "        onnx_model_path: Path to the input ONNX model\n",
        "        output_path: Path for the quantized output model\n",
        "        calibration_data_reader: Data reader for calibration\n",
        "        nodes_to_exclude: List of nodes to exclude from quantization\n",
        "    \"\"\"\n",
        "    print(f\"Quantizing ONNX model to INT8: {output_path}\")\n",
        "\n",
        "    # First verify the ONNX model can be loaded\n",
        "    try:\n",
        "        # Check that model exists\n",
        "        if not os.path.exists(onnx_model_path):\n",
        "            raise FileNotFoundError(f\"ONNX model not found: {onnx_model_path}\")\n",
        "\n",
        "        # Load and verify model\n",
        "        onnx_model = onnx.load(onnx_model_path)\n",
        "        print(f\"✓ ONNX model loaded successfully. Model IR version: {onnx_model.ir_version}\")\n",
        "\n",
        "        # Get model inputs\n",
        "        input_name = onnx_model.graph.input[0].name\n",
        "        print(f\"Model input name: {input_name}\")\n",
        "\n",
        "        # Update input name in calibration reader if needed\n",
        "        if hasattr(calibration_data_reader, 'input_name') and calibration_data_reader.input_name != input_name:\n",
        "            print(f\"⚠ Updating input name in calibration reader from '{calibration_data_reader.input_name}' to '{input_name}'\")\n",
        "            calibration_data_reader.input_name = input_name\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error loading ONNX model: {e}\")\n",
        "        print(\"Attempting to continue with quantization anyway...\")\n",
        "\n",
        "    # Check ONNX Runtime version\n",
        "    print(f\"ONNX Runtime version: {ort.__version__}\")\n",
        "\n",
        "    # Determine which op types to quantize\n",
        "    # Note: excluding complex operations that might be sensitive to quantization\n",
        "    op_types_to_quantize = [\n",
        "        'Conv', 'MatMul', 'Gemm', 'Add', 'Mul',\n",
        "        'Concat', 'MaxPool', 'AveragePool', 'Resize'\n",
        "    ]\n",
        "\n",
        "    # Extra options for calibration method\n",
        "    extra_options = {}\n",
        "    if Config.CALIBRATION_METHOD == CalibrationMethod.Percentile:\n",
        "        extra_options = {\"percentile\": Config.PERCENTILE}\n",
        "\n",
        "    # Try to apply quantization with different approaches if needed\n",
        "    try:\n",
        "        print(f\"Starting quantization with {Config.CALIBRATION_METHOD} calibration method...\")\n",
        "        print(f\"Using quantization format: {Config.QUANT_FORMAT}\")\n",
        "\n",
        "        # Apply quantization with calibration\n",
        "        quantize_static(\n",
        "            model_input=onnx_model_path,\n",
        "            model_output=output_path,\n",
        "            calibration_data_reader=calibration_data_reader,\n",
        "            quant_format=Config.QUANT_FORMAT,  # Now using QOperator format\n",
        "            weight_type=Config.WEIGHT_TYPE,\n",
        "            activation_type=Config.ACTIVATION_TYPE,\n",
        "            per_channel=Config.PER_CHANNEL,\n",
        "            reduce_range=False,  # Modern hardware usually doesn't need this\n",
        "            calibrate_method=Config.CALIBRATION_METHOD,\n",
        "            nodes_to_exclude=nodes_to_exclude,\n",
        "            op_types_to_quantize=op_types_to_quantize,\n",
        "            extra_options=extra_options,\n",
        "            optimize_model=True\n",
        "        )\n",
        "        print(\"✓ Quantization complete with primary method.\")\n",
        "\n",
        "        # Verify the model can be loaded\n",
        "        try:\n",
        "            test_session = ort.InferenceSession(output_path)\n",
        "            print(\"✓ Quantized model verified to load correctly\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Quantized model verification failed: {e}\")\n",
        "            print(\"Will try alternative quantization approach...\")\n",
        "            raise ValueError(\"Model verification failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Primary quantization failed: {e}\")\n",
        "        print(\"Trying alternative quantization method...\")\n",
        "\n",
        "        try:\n",
        "            # Use dynamic quantization as a fallback\n",
        "            print(\"Trying dynamic quantization without calibration...\")\n",
        "            from onnxruntime.quantization.quantize import quantize_dynamic\n",
        "\n",
        "            quantize_dynamic(\n",
        "                model_input=onnx_model_path,\n",
        "                model_output=output_path,\n",
        "                weight_type=Config.WEIGHT_TYPE,\n",
        "                op_types_to_quantize=op_types_to_quantize,\n",
        "                nodes_to_exclude=nodes_to_exclude\n",
        "            )\n",
        "            print(\"✓ Dynamic quantization complete (fallback method).\")\n",
        "\n",
        "            # Verify the model can be loaded\n",
        "            try:\n",
        "                test_session = ort.InferenceSession(output_path)\n",
        "                print(\"✓ Dynamically quantized model verified to load correctly\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Dynamically quantized model verification failed: {e}\")\n",
        "                print(\"Will fall back to model optimization without quantization...\")\n",
        "                raise ValueError(\"Model verification failed\")\n",
        "\n",
        "        except Exception as e2:\n",
        "            print(f\"⚠ Dynamic quantization also failed: {e2}\")\n",
        "            print(\"Trying ONNX Runtime optimization without quantization...\")\n",
        "\n",
        "            try:\n",
        "                # Configure session options for optimization\n",
        "                sess_options = ort.SessionOptions()\n",
        "                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "                optimized_model_path = output_path.replace('.onnx', '_optimized.onnx')\n",
        "                sess_options.optimized_model_filepath = optimized_model_path\n",
        "\n",
        "                # Create session to optimize the model\n",
        "                _ = ort.InferenceSession(onnx_model_path, sess_options)\n",
        "\n",
        "                if os.path.exists(optimized_model_path):\n",
        "                    print(f\"✓ Model optimization completed successfully: {optimized_model_path}\")\n",
        "                    # Copy optimized model to intended output path\n",
        "                    import shutil\n",
        "                    shutil.copy(optimized_model_path, output_path)\n",
        "                    print(f\"✓ Optimized model copied to: {output_path}\")\n",
        "                else:\n",
        "                    print(f\"⚠ Model optimization failed, output file not found\")\n",
        "                    # Copy original model to output path\n",
        "                    import shutil\n",
        "                    shutil.copy(onnx_model_path, output_path)\n",
        "                    print(f\"✓ Copied original model to: {output_path}\")\n",
        "            except Exception as e3:\n",
        "                print(f\"⚠ All optimization methods failed: {e3}\")\n",
        "                print(\"Providing original model without optimization\")\n",
        "\n",
        "                # Copy original model to output path\n",
        "                import shutil\n",
        "                shutil.copy(onnx_model_path, output_path)\n",
        "                print(f\"✓ Copied original model to: {output_path}\")\n",
        "\n",
        "    # Verify the final model\n",
        "    if os.path.exists(output_path):\n",
        "        try:\n",
        "            quant_model = onnx.load(output_path)\n",
        "            print(f\"✓ Final model verified and saved to: {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Warning: Final model verification failed: {e}\")\n",
        "    else:\n",
        "        print(f\"⚠ Warning: Final model file not found at {output_path}\")\n",
        "\n",
        "# =====================================\n",
        "# PART 4: MODEL EVALUATION\n",
        "# =====================================\n",
        "\n",
        "def preprocess_image(\n",
        "    image_path: str,\n",
        "    input_size: Tuple[int, int] = (640, 640)\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Preprocess an image for YOLO inference with robust error handling\n",
        "    Args:\n",
        "        image_path: Path to the image\n",
        "        input_size: Model input size (width, height)\n",
        "    Returns:\n",
        "        Tuple containing the preprocessed image and the original image\n",
        "    \"\"\"\n",
        "    # Check if file exists first\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"Image file does not exist: {image_path}\")\n",
        "\n",
        "    # Try reading with cv2\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # If reading fails, try an alternative approach\n",
        "    if img is None:\n",
        "        print(f\"Warning: cv2.imread failed for {image_path}, trying alternative loading method\")\n",
        "        try:\n",
        "            # Try using PIL instead\n",
        "            from PIL import Image\n",
        "            pil_img = Image.open(image_path)\n",
        "            img = np.array(pil_img)\n",
        "            if pil_img.mode == 'RGB':\n",
        "                # Convert RGB to BGR for cv2 compatibility\n",
        "                img = img[:, :, ::-1].copy()\n",
        "        except Exception as e:\n",
        "            print(f\"Alternative loading method also failed: {e}\")\n",
        "            # Use a dummy image as a last resort\n",
        "            print(\"Creating a dummy 640x640 image for testing purposes\")\n",
        "            img = np.zeros((640, 640, 3), dtype=np.uint8)\n",
        "            img[100:200, 100:200, 0] = 255  # Add a red rectangle as a test pattern\n",
        "\n",
        "    original_img = img.copy()\n",
        "\n",
        "    # Resize\n",
        "    img = cv2.resize(img, input_size)\n",
        "\n",
        "    # Normalize and convert to proper format\n",
        "    img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
        "    img = img.transpose(2, 0, 1)  # HWC to CHW\n",
        "    img = np.expand_dims(img, 0)  # Add batch dimension\n",
        "\n",
        "    return img, original_img\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model_path: str,\n",
        "    test_images_dir: str,\n",
        "    conf_threshold: float = 0.25,\n",
        "    iou_threshold: float = 0.45\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Evaluate the model's inference performance\n",
        "    Args:\n",
        "        model_path: Path to the ONNX model\n",
        "        test_images_dir: Directory containing test images\n",
        "        conf_threshold: Confidence threshold for detection\n",
        "        iou_threshold: IoU threshold for NMS\n",
        "    Returns:\n",
        "        Dictionary with performance metrics\n",
        "    \"\"\"\n",
        "    print(f\"Evaluating model: {model_path}\")\n",
        "\n",
        "    # Create ONNX Runtime session\n",
        "    session_options = ort.SessionOptions()\n",
        "    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session_options.intra_op_num_threads = Config.NUM_THREADS\n",
        "\n",
        "    try:\n",
        "        session = ort.InferenceSession(\n",
        "            model_path,\n",
        "            sess_options=session_options,\n",
        "            providers=['CPUExecutionProvider']  # For bodycam deployment\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error creating inference session: {e}\")\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "    # Get input details\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    input_shape = session.get_inputs()[0].shape\n",
        "    if len(input_shape) == 4:\n",
        "        _, _, height, width = input_shape\n",
        "    else:\n",
        "        height, width = Config.INPUT_SIZE\n",
        "\n",
        "    # Get output details\n",
        "    output_name = session.get_outputs()[0].name\n",
        "\n",
        "    # Get all test images\n",
        "    image_paths = []\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:\n",
        "        image_paths.extend(glob(os.path.join(test_images_dir, ext)))\n",
        "\n",
        "    if not image_paths:\n",
        "        print(f\"⚠ No images found in {test_images_dir}\")\n",
        "        # Use dummy images for testing\n",
        "        print(\"Creating dummy evaluation data...\")\n",
        "        return {\"avg_inference_time_ms\": 0.0, \"fps\": 0.0, \"error\": \"No test images found\"}\n",
        "\n",
        "    print(f\"Found {len(image_paths)} test images\")\n",
        "\n",
        "    # Performance metrics\n",
        "    inference_times = []\n",
        "\n",
        "    # Process each image\n",
        "    valid_images = 0\n",
        "    for img_path in tqdm(image_paths[:10]):  # Limit to 10 images for quick evaluation\n",
        "        try:\n",
        "            # Preprocess image\n",
        "            input_data, original_img = preprocess_image(\n",
        "                img_path,\n",
        "                input_size=(width, height)\n",
        "            )\n",
        "\n",
        "            # Run inference with timing\n",
        "            start_time = time.time()\n",
        "            outputs = session.run([output_name], {input_name: input_data})\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            inference_times.append(inference_time)\n",
        "            valid_images += 1\n",
        "\n",
        "            if valid_images >= 3:  # Stop after successfully processing 3 images\n",
        "                break\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error processing {img_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not inference_times:\n",
        "        print(\"⚠ No valid images were processed during evaluation\")\n",
        "        return {\"avg_inference_time_ms\": 0.0, \"fps\": 0.0, \"error\": \"No valid images for evaluation\"}\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    fps = 1.0 / avg_inference_time\n",
        "\n",
        "    print(f\"Average inference time: {avg_inference_time*1000:.2f} ms\")\n",
        "    print(f\"Frames per second: {fps:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"avg_inference_time_ms\": avg_inference_time * 1000,\n",
        "        \"fps\": fps\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_models(\n",
        "    fp32_model_path: str,\n",
        "    int8_model_path: str,\n",
        "    test_image_path: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compare detection results between FP32 and INT8 models\n",
        "    Args:\n",
        "        fp32_model_path: Path to the FP32 ONNX model\n",
        "        int8_model_path: Path to the INT8 ONNX model\n",
        "        test_image_path: Path to a test image\n",
        "    \"\"\"\n",
        "    print(\"Comparing model outputs...\")\n",
        "\n",
        "    try:\n",
        "        # Create inference sessions\n",
        "        fp32_session = ort.InferenceSession(\n",
        "            fp32_model_path,\n",
        "            providers=['CPUExecutionProvider']\n",
        "        )\n",
        "\n",
        "        int8_session = ort.InferenceSession(\n",
        "            int8_model_path,\n",
        "            providers=['CPUExecutionProvider']\n",
        "        )\n",
        "\n",
        "        # Get input details\n",
        "        input_name = fp32_session.get_inputs()[0].name\n",
        "        input_shape = fp32_session.get_inputs()[0].shape\n",
        "        if len(input_shape) == 4:\n",
        "            _, _, height, width = input_shape\n",
        "        else:\n",
        "            height, width = Config.INPUT_SIZE\n",
        "\n",
        "        # Preprocess image\n",
        "        input_data, original_img = preprocess_image(\n",
        "            test_image_path,\n",
        "            input_size=(width, height)\n",
        "        )\n",
        "\n",
        "        # Run inference on both models\n",
        "        fp32_outputs = fp32_session.run(None, {input_name: input_data})\n",
        "        int8_outputs = int8_session.run(None, {input_name: input_data})\n",
        "\n",
        "        # Compare output shapes and values\n",
        "        print(\"Output comparison:\")\n",
        "        for i, (fp32_out, int8_out) in enumerate(zip(fp32_outputs, int8_outputs)):\n",
        "            print(f\"Output {i}:\")\n",
        "            print(f\"  FP32 shape: {fp32_out.shape}\")\n",
        "            print(f\"  INT8 shape: {int8_out.shape}\")\n",
        "\n",
        "            if fp32_out.shape == int8_out.shape:\n",
        "                abs_diff = np.abs(fp32_out - int8_out)\n",
        "                max_diff = np.max(abs_diff)\n",
        "                mean_diff = np.mean(abs_diff)\n",
        "\n",
        "                print(f\"  Max absolute difference: {max_diff}\")\n",
        "                print(f\"  Mean absolute difference: {mean_diff}\")\n",
        "\n",
        "                # Calculate relative error\n",
        "                rel_error = np.mean(abs_diff / (np.abs(fp32_out) + 1e-10))\n",
        "                print(f\"  Mean relative error: {rel_error:.6f}\")\n",
        "\n",
        "                # Report overall match quality\n",
        "                if rel_error < 0.01:\n",
        "                    print(\"  Quality: Excellent match\")\n",
        "                elif rel_error < 0.05:\n",
        "                    print(\"  Quality: Good match\")\n",
        "                elif rel_error < 0.1:\n",
        "                    print(\"  Quality: Fair match\")\n",
        "                else:\n",
        "                    print(\"  Quality: Poor match\")\n",
        "            else:\n",
        "                print(\"  ERROR: Output shapes don't match\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error comparing models: {e}\")\n",
        "        print(\"Continue without comparison\")\n",
        "\n",
        "# =====================================\n",
        "# PART 5: OPTIMIZED INFERENCE FOR BODYCAM\n",
        "# =====================================\n",
        "\n",
        "class OptimizedInferenceEngine:\n",
        "    \"\"\"\n",
        "    Optimized inference engine for bodycam deployment\n",
        "    Handles memory-efficient inference with INT8 model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        input_size: Tuple[int, int] = (640, 640),\n",
        "        conf_threshold: float = 0.25,\n",
        "        iou_threshold: float = 0.45,\n",
        "        num_threads: int = 2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the optimized inference engine\n",
        "        Args:\n",
        "            model_path: Path to the ONNX model\n",
        "            input_size: Model input size (width, height)\n",
        "            conf_threshold: Confidence threshold for detection\n",
        "            iou_threshold: IoU threshold for NMS\n",
        "            num_threads: Number of CPU threads for inference\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.conf_threshold = conf_threshold\n",
        "        self.iou_threshold = iou_threshold\n",
        "\n",
        "        # Configure session options\n",
        "        session_options = ort.SessionOptions()\n",
        "        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
        "        session_options.intra_op_num_threads = num_threads\n",
        "        session_options.enable_mem_pattern = True\n",
        "        session_options.enable_mem_reuse = True\n",
        "\n",
        "        # Provider options for CPU\n",
        "        provider_options = {\n",
        "            'arena_extend_strategy': 'kSameAsRequested',\n",
        "            'cpu_memory_arena_cfg': '16384',  # 16MB arena (adjust based on hardware)\n",
        "        }\n",
        "\n",
        "        # Create inference session\n",
        "        self.session = ort.InferenceSession(\n",
        "            model_path,\n",
        "            sess_options=session_options,\n",
        "            providers=[('CPUExecutionProvider', provider_options)]\n",
        "        )\n",
        "\n",
        "        # Get input and output details\n",
        "        self.input_name = self.session.get_inputs()[0].name\n",
        "        self.output_name = self.session.get_outputs()[0].name\n",
        "\n",
        "        # Pre-allocate input/output buffers\n",
        "        self.input_buffer = np.zeros((1, 3, input_size[1], input_size[0]), dtype=np.float32)\n",
        "\n",
        "        # For binding memory\n",
        "        self.io_binding = self.session.io_binding()\n",
        "        print(f\"Optimized inference engine initialized for {model_path}\")\n",
        "\n",
        "    def preprocess(self, frame: np.ndarray) -> Tuple[np.ndarray, float, float]:\n",
        "        \"\"\"\n",
        "        Preprocess a frame for inference\n",
        "        Args:\n",
        "            frame: Input frame (BGR format)\n",
        "        Returns:\n",
        "            Tuple containing the preprocessed frame and scaling factors\n",
        "        \"\"\"\n",
        "        # Get original dimensions\n",
        "        original_height, original_width = frame.shape[:2]\n",
        "\n",
        "        # Calculate scale factors\n",
        "        scale_x = self.input_size[0] / original_width\n",
        "        scale_y = self.input_size[1] / original_height\n",
        "\n",
        "        # Resize\n",
        "        resized = cv2.resize(frame, self.input_size)\n",
        "\n",
        "        # Convert to float32 and normalize (in-place operations)\n",
        "        preprocessed = resized.astype(np.float32) / 255.0\n",
        "\n",
        "        # Transpose and copy to input buffer (avoid memory allocation)\n",
        "        # HWC to CHW format\n",
        "        self.input_buffer[0, 0, :, :] = preprocessed[:, :, 2]  # B\n",
        "        self.input_buffer[0, 1, :, :] = preprocessed[:, :, 1]  # G\n",
        "        self.input_buffer[0, 2, :, :] = preprocessed[:, :, 0]  # R\n",
        "\n",
        "        return self.input_buffer, scale_x, scale_y\n",
        "\n",
        "    def infer(self, frame: np.ndarray) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Run inference on a frame with optimized memory usage\n",
        "        Args:\n",
        "            frame: Input frame (BGR format)\n",
        "        Returns:\n",
        "            List of detection results (boxes, scores, classes)\n",
        "        \"\"\"\n",
        "        # Preprocess frame\n",
        "        input_data, scale_x, scale_y = self.preprocess(frame)\n",
        "\n",
        "        # Use IO binding for faster inference\n",
        "        self.io_binding.bind_input(\n",
        "            name=self.input_name,\n",
        "            device_type=ort.OrtDevice.cpu(),\n",
        "            device_id=0,\n",
        "            element_type=np.float32,\n",
        "            shape=input_data.shape,\n",
        "            buffer_ptr=input_data.ctypes.data\n",
        "        )\n",
        "\n",
        "        self.io_binding.bind_output(self.output_name)\n",
        "\n",
        "        # Run inference\n",
        "        self.session.run_with_iobinding(self.io_binding)\n",
        "\n",
        "        # Get output\n",
        "        outputs = self.io_binding.get_outputs()[0]\n",
        "        output_data = outputs.numpy()\n",
        "\n",
        "        # Process detections (YOLO format)\n",
        "        detections = self.process_output(output_data, frame.shape[1], frame.shape[0], scale_x, scale_y)\n",
        "\n",
        "        return detections\n",
        "\n",
        "    def process_output(\n",
        "        self,\n",
        "        output: np.ndarray,\n",
        "        original_width: int,\n",
        "        original_height: int,\n",
        "        scale_x: float,\n",
        "        scale_y: float\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Process model output to get detection results\n",
        "        Args:\n",
        "            output: Model output\n",
        "            original_width: Original frame width\n",
        "            original_height: Original frame height\n",
        "            scale_x: X-axis scaling factor\n",
        "            scale_y: Y-axis scaling factor\n",
        "        Returns:\n",
        "            List of detection results\n",
        "        \"\"\"\n",
        "        # Process based on YOLO output format\n",
        "        # This assumes a standard YOLO output format, may need adjustment for YOLOv12n\n",
        "        results = []\n",
        "\n",
        "        # Apply confidence threshold\n",
        "        mask = output[..., 4] > self.conf_threshold\n",
        "        detections = output[mask]\n",
        "\n",
        "        if len(detections) > 0:\n",
        "            # Extract boxes, scores, and classes\n",
        "            boxes = detections[:, 0:4]\n",
        "            scores = detections[:, 4]\n",
        "            classes = detections[:, 5:]\n",
        "            class_ids = np.argmax(classes, axis=1)\n",
        "\n",
        "            # Convert boxes to original image coordinates\n",
        "            boxes[:, 0] /= scale_x\n",
        "            boxes[:, 2] /= scale_x\n",
        "            boxes[:, 1] /= scale_y\n",
        "            boxes[:, 3] /= scale_y\n",
        "\n",
        "            # Ensure boxes are within image bounds\n",
        "            boxes[:, 0] = np.clip(boxes[:, 0], 0, original_width)\n",
        "            boxes[:, 1] = np.clip(boxes[:, 1], 0, original_height)\n",
        "            boxes[:, 2] = np.clip(boxes[:, 2], 0, original_width)\n",
        "            boxes[:, 3] = np.clip(boxes[:, 3], 0, original_height)\n",
        "\n",
        "            # Create result list\n",
        "            for box, score, class_id in zip(boxes, scores, class_ids):\n",
        "                results.append({\n",
        "                    'bbox': box.tolist(),\n",
        "                    'score': float(score),\n",
        "                    'class_id': int(class_id)\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "# =====================================\n",
        "# PART 6: MAIN EXECUTION FLOW\n",
        "# =====================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution flow for model export and quantization\n",
        "    \"\"\"\n",
        "    # Setup directories and mount Google Drive\n",
        "    Config.setup()\n",
        "\n",
        "    # 1. Load and prepare model\n",
        "    try:\n",
        "        model = prepare_model(Config.MODEL_PT_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error loading model: {e}\")\n",
        "        print(\"Trying to continue with export anyway...\")\n",
        "\n",
        "        # Create a placeholder model if needed for debugging\n",
        "        print(\"Do you want to continue without a properly loaded model? (y/n)\")\n",
        "        response = input().strip().lower()\n",
        "        if response != 'y':\n",
        "            print(\"Exiting script...\")\n",
        "            return\n",
        "\n",
        "    # 2. Export to ONNX\n",
        "    try:\n",
        "        export_to_onnx(model, Config.ONNX_MODEL_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error exporting to ONNX: {e}\")\n",
        "\n",
        "        # Check if export was done directly by YOLO\n",
        "        if os.path.exists(Config.ONNX_MODEL_PATH):\n",
        "            print(f\"However, ONNX file exists at {Config.ONNX_MODEL_PATH}. Continuing...\")\n",
        "        else:\n",
        "            # Try using ultralytics export\n",
        "            print(\"Trying direct export through YOLO...\")\n",
        "            try:\n",
        "                yolo_model = YOLO(Config.MODEL_PT_PATH)\n",
        "                export_path = os.path.join(Config.ONNX_OUTPUT_DIR, \"yolov12n_enhanced.onnx\")\n",
        "                yolo_model.export(format=\"onnx\", imgsz=Config.INPUT_SIZE, opset=Config.OPSET_VERSION,\n",
        "                                  half=False, simplify=True, dynamic=True, batch=Config.BATCH_SIZE)\n",
        "\n",
        "                # Check if export was successful and update path\n",
        "                if os.path.exists(export_path):\n",
        "                    Config.ONNX_MODEL_PATH = export_path\n",
        "                    print(f\"✓ Model exported successfully using YOLO: {Config.ONNX_MODEL_PATH}\")\n",
        "                else:\n",
        "                    print(f\"⚠ YOLO export failed: File not found at {export_path}\")\n",
        "                    print(\"Checking for other ONNX exports in the directory...\")\n",
        "\n",
        "                    # Check if any ONNX file was created\n",
        "                    onnx_files = glob(os.path.join(os.path.dirname(Config.MODEL_PT_PATH), \"*.onnx\"))\n",
        "                    if onnx_files:\n",
        "                        Config.ONNX_MODEL_PATH = onnx_files[0]\n",
        "                        print(f\"✓ Found ONNX model: {Config.ONNX_MODEL_PATH}\")\n",
        "                    else:\n",
        "                        print(\"No ONNX models found. Cannot continue.\")\n",
        "                        return\n",
        "            except Exception as e2:\n",
        "                print(f\"⚠ YOLO export also failed: {e2}\")\n",
        "                print(\"Cannot proceed without an ONNX model. Exiting...\")\n",
        "                return\n",
        "\n",
        "    # 3. Verify ONNX model\n",
        "    try:\n",
        "        if 'model' in locals():\n",
        "            verify_onnx_model(Config.ONNX_MODEL_PATH, model)\n",
        "        else:\n",
        "            print(\"Skipping verification as model wasn't loaded properly\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error verifying ONNX model: {e}\")\n",
        "        print(\"Continuing with caution...\")\n",
        "\n",
        "    # 4. Identify attention nodes to exclude from quantization\n",
        "    try:\n",
        "        nodes_to_exclude = identify_attention_nodes(Config.ONNX_MODEL_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Error identifying attention nodes: {e}\")\n",
        "        nodes_to_exclude = []\n",
        "        print(\"Continuing without excluding nodes...\")\n",
        "\n",
        "    # Ask the user if they want to use normal calibration or direct quantization\n",
        "    print(\"\\nDo you want to use calibration-based quantization or direct quantization?\")\n",
        "    print(\"[1] Calibration-based (more accurate but requires valid images)\")\n",
        "    print(\"[2] Direct quantization (faster, no calibration data needed)\")\n",
        "    quant_choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "\n",
        "    if quant_choice == \"2\":\n",
        "        # Direct quantization path\n",
        "        print(\"Using direct quantization without calibration...\")\n",
        "        success = direct_quantization(\n",
        "            Config.ONNX_MODEL_PATH,\n",
        "            Config.ONNX_INT8_MODEL_PATH,\n",
        "            nodes_to_exclude\n",
        "        )\n",
        "\n",
        "        if not success:\n",
        "            print(\"Direct quantization failed. Exiting...\")\n",
        "            return\n",
        "    else:\n",
        "        # Standard calibration-based quantization path\n",
        "        try:\n",
        "            # Check if calibration directory exists and has images\n",
        "            if not os.path.exists(Config.CALIBRATION_DATA_DIR):\n",
        "                print(f\"⚠ Calibration directory not found: {Config.CALIBRATION_DATA_DIR}\")\n",
        "                # Look for alternative calibration data\n",
        "                print(\"Looking for alternative calibration data...\")\n",
        "\n",
        "                possible_dirs = [\n",
        "                    '/content/drive/MyDrive/SemesterProjectDatas/CombinedData/images',\n",
        "                    '/content/drive/MyDrive/SemesterProjectDatas/CombinedData/train/images',\n",
        "                    '/content/drive/MyDrive/SemesterProjectDatas/CombinedData/test/images',\n",
        "                    '/content/drive/MyDrive/SemesterProjectDatas/images',\n",
        "                    '/content/drive/My Drive/SemesterProjectDatas/CombinedData/images',\n",
        "                    '/content/drive/My Drive/SemesterProjectDatas/CombinedData/train/images',\n",
        "                    '/content/drive/My Drive/SemesterProjectDatas/CombinedData/test/images',\n",
        "                    '/content/drive/My Drive/SemesterProjectDatas/images',\n",
        "                    '/content/drive/My Drive/SemesterProjectDatas/CombinedData/valid/images',\n",
        "                    '/content/drive/My Drive/SemesterProjectDatas/CombinedData/val/images'\n",
        "                ]\n",
        "\n",
        "                for directory in possible_dirs:\n",
        "                    if os.path.exists(directory) and any(glob(os.path.join(directory, '*.jpg'))):\n",
        "                        Config.CALIBRATION_DATA_DIR = directory\n",
        "                        print(f\"✓ Found alternative calibration directory: {directory}\")\n",
        "                        break\n",
        "                else:\n",
        "                    print(\"No suitable calibration directories found. Trying direct quantization instead...\")\n",
        "                    success = direct_quantization(\n",
        "                        Config.ONNX_MODEL_PATH,\n",
        "                        Config.ONNX_INT8_MODEL_PATH,\n",
        "                        nodes_to_exclude\n",
        "                    )\n",
        "\n",
        "                    if not success:\n",
        "                        print(\"Direct quantization failed. Exiting...\")\n",
        "                        return\n",
        "                    else:\n",
        "                        print(\"Proceeding with direct quantization results...\")\n",
        "\n",
        "            if Config.CALIBRATION_DATA_DIR and os.path.exists(Config.CALIBRATION_DATA_DIR):\n",
        "                # Create the calibration data reader\n",
        "                try:\n",
        "                    print(f\"Creating calibration data reader for: {Config.CALIBRATION_DATA_DIR}\")\n",
        "                    calibration_data_reader = YOLOCalibrationDataReader(\n",
        "                        Config.CALIBRATION_DATA_DIR,\n",
        "                        \"input\",\n",
        "                        Config.INPUT_SIZE,\n",
        "                        Config.NUM_CALIBRATION_IMAGES\n",
        "                    )\n",
        "\n",
        "                    # Quantize model with calibration\n",
        "                    quantize_onnx_model(\n",
        "                        Config.ONNX_MODEL_PATH,\n",
        "                        Config.ONNX_INT8_MODEL_PATH,\n",
        "                        calibration_data_reader,\n",
        "                        nodes_to_exclude\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠ Error during calibration: {e}\")\n",
        "                    print(\"Falling back to direct quantization...\")\n",
        "                    success = direct_quantization(\n",
        "                        Config.ONNX_MODEL_PATH,\n",
        "                        Config.ONNX_INT8_MODEL_PATH,\n",
        "                        nodes_to_exclude\n",
        "                    )\n",
        "\n",
        "                    if not success:\n",
        "                        print(\"Direct quantization failed. Exiting...\")\n",
        "                        return\n",
        "                    else:\n",
        "                        print(\"Proceeding with direct quantization results...\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error creating calibration data reader: {e}\")\n",
        "            print(\"Falling back to direct quantization...\")\n",
        "            success = direct_quantization(\n",
        "                Config.ONNX_MODEL_PATH,\n",
        "                Config.ONNX_INT8_MODEL_PATH,\n",
        "                nodes_to_exclude\n",
        "            )\n",
        "\n",
        "            if not success:\n",
        "                print(\"Direct quantization failed. Exiting...\")\n",
        "                return\n",
        "            else:\n",
        "                print(\"Proceeding with direct quantization results...\")\n",
        "\n",
        "    # 7. Compare model outputs\n",
        "    if os.path.exists(Config.EVAL_DATA_DIR) and os.listdir(Config.EVAL_DATA_DIR):\n",
        "        try:\n",
        "            # Find a valid image for comparison\n",
        "            valid_image = None\n",
        "            for img_file in os.listdir(Config.EVAL_DATA_DIR)[:10]:  # Try first 10 images\n",
        "                img_path = os.path.join(Config.EVAL_DATA_DIR, img_file)\n",
        "                try:\n",
        "                    # Test if we can load the image\n",
        "                    test_img = cv2.imread(img_path)\n",
        "                    if test_img is not None:\n",
        "                        valid_image = img_path\n",
        "                        print(f\"Found valid image for comparison: {valid_image}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if valid_image:\n",
        "                compare_models(Config.ONNX_MODEL_PATH, Config.ONNX_INT8_MODEL_PATH, valid_image)\n",
        "            else:\n",
        "                print(\"No valid images found for comparison\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error comparing models: {e}\")\n",
        "            print(\"Continuing without comparison...\")\n",
        "\n",
        "    # 8. Evaluate performance\n",
        "    if os.path.exists(Config.EVAL_DATA_DIR) and os.listdir(Config.EVAL_DATA_DIR):\n",
        "        try:\n",
        "            # Evaluate FP32 model\n",
        "            print(\"\\nEvaluating FP32 model:\")\n",
        "            fp32_metrics = evaluate_model(\n",
        "                Config.ONNX_MODEL_PATH,\n",
        "                Config.EVAL_DATA_DIR,\n",
        "                Config.CONFIDENCE_THRESHOLD,\n",
        "                Config.IOU_THRESHOLD\n",
        "            )\n",
        "\n",
        "            # Evaluate INT8 model\n",
        "            print(\"\\nEvaluating INT8 model:\")\n",
        "            int8_metrics = evaluate_model(\n",
        "                Config.ONNX_INT8_MODEL_PATH,\n",
        "                Config.EVAL_DATA_DIR,\n",
        "                Config.CONFIDENCE_THRESHOLD,\n",
        "                Config.IOU_THRESHOLD\n",
        "            )\n",
        "\n",
        "            # Calculate speedup\n",
        "            if 'avg_inference_time_ms' in fp32_metrics and 'avg_inference_time_ms' in int8_metrics and fp32_metrics['avg_inference_time_ms'] > 0:\n",
        "                speedup = fp32_metrics[\"avg_inference_time_ms\"] / int8_metrics[\"avg_inference_time_ms\"]\n",
        "                print(f\"\\nINT8 speedup: {speedup:.2f}x\")\n",
        "            else:\n",
        "                speedup = 0\n",
        "                print(\"\\nCould not calculate speedup due to missing metrics\")\n",
        "\n",
        "            # Save metrics\n",
        "            all_metrics = {\n",
        "                \"fp32\": fp32_metrics,\n",
        "                \"int8\": int8_metrics,\n",
        "                \"speedup\": speedup\n",
        "            }\n",
        "\n",
        "            metrics_path = os.path.join(Config.ONNX_OUTPUT_DIR, \"quantization_metrics.json\")\n",
        "            with open(metrics_path, 'w') as f:\n",
        "                json.dump(all_metrics, f, indent=4)\n",
        "\n",
        "            print(f\"Metrics saved to {metrics_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error evaluating models: {e}\")\n",
        "            print(\"Continuing without evaluation...\")\n",
        "\n",
        "    print(\"\\nModel export and quantization process complete!\")\n",
        "    print(f\"FP32 ONNX model: {Config.ONNX_MODEL_PATH}\")\n",
        "    print(f\"INT8 ONNX model: {Config.ONNX_INT8_MODEL_PATH}\")\n",
        "\n",
        "    # Optional: Save a copy of the best model for easier access\n",
        "    best_onnx_path = os.path.join(Config.ONNX_OUTPUT_DIR, \"best_model_int8.onnx\")\n",
        "    try:\n",
        "        import shutil\n",
        "        shutil.copy(Config.ONNX_INT8_MODEL_PATH, best_onnx_path)\n",
        "        print(f\"Copied best INT8 model to: {best_onnx_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not copy best model: {e}\")\n",
        "\n",
        "# =====================================\n",
        "# PART 7: EXAMPLE USAGE\n",
        "# =====================================\n",
        "\n",
        "def example_usage():\n",
        "    \"\"\"\n",
        "    Example of how to use the quantized model for inference\n",
        "    \"\"\"\n",
        "    # Path to quantized model\n",
        "    model_path = Config.ONNX_INT8_MODEL_PATH\n",
        "\n",
        "    # Create optimized inference engine\n",
        "    engine = OptimizedInferenceEngine(\n",
        "        model_path,\n",
        "        input_size=Config.INPUT_SIZE,\n",
        "        conf_threshold=Config.CONFIDENCE_THRESHOLD,\n",
        "        iou_threshold=Config.IOU_THRESHOLD,\n",
        "        num_threads=Config.NUM_THREADS\n",
        "    )\n",
        "\n",
        "    # For camera input\n",
        "    cap = cv2.VideoCapture(0)  # For bodycam, this would be the camera device ID\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            # Read frame\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Run optimized inference\n",
        "            detections = engine.infer(frame)\n",
        "\n",
        "            # Visualize detections\n",
        "            for det in detections:\n",
        "                box = det['bbox']\n",
        "                score = det['score']\n",
        "                class_id = det['class_id']\n",
        "\n",
        "                # Convert box coordinates to int\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "                # Draw bounding box\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "                # Draw label\n",
        "                label = f\"Class {class_id}: {score:.2f}\"\n",
        "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "            # Display frame\n",
        "            cv2.imshow('YOLO Detections', frame)\n",
        "\n",
        "            # Exit on 'q' press\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    finally:\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "# Complete standalone script for Colab usage - just run this cell\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the full export and quantization pipeline\n",
        "    main()\n",
        "\n",
        "    # Print a final summary for easy copy-paste of model paths\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Original PyTorch model: {Config.MODEL_PT_PATH}\")\n",
        "    print(f\"FP32 ONNX model: {Config.ONNX_MODEL_PATH}\")\n",
        "    print(f\"INT8 ONNX model: {Config.ONNX_INT8_MODEL_PATH}\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"To use the INT8 model in your application, use the following:\")\n",
        "    print(\"```python\")\n",
        "    print(\"import onnxruntime as ort\")\n",
        "    print(f\"session = ort.InferenceSession(\\\"{Config.ONNX_INT8_MODEL_PATH}\\\")\")\n",
        "    print(\"# Run inference with:\")\n",
        "    print(\"# outputs = session.run(None, {\\\"input\\\": preprocessed_image})\")\n",
        "    print(\"```\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Optionally, try to load and validate the model (quietly skipped if it fails)\n",
        "    try:\n",
        "        # Verify that the quantized model can be loaded\n",
        "        quantized_session = ort.InferenceSession(Config.ONNX_INT8_MODEL_PATH)\n",
        "        print(\"✓ Successfully verified that the quantized model can be loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Could not verify the quantized model: {e}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFD6NRxtQqtn",
        "outputId": "3f45b3da-32ae-4a48-a826-161bd069b249"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying Google Drive mount...\n",
            "✓ Google Drive mounted successfully\n",
            "Project base directory: /content/drive/My Drive/SemesterProjectDatas\n",
            "Output directory created/verified: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized\n",
            "\n",
            "Searching for image directories in the project...\n",
            "\n",
            "Found directories with images:\n",
            "  [0] /content/drive/My Drive/SemesterProjectDatas/ManualRecording (1 images)\n",
            "  [1] /content/drive/My Drive/SemesterProjectDatas/TACO/train/images (3146 images)\n",
            "  [2] /content/drive/My Drive/SemesterProjectDatas/TACO/test/images (150 images)\n",
            "  [3] /content/drive/My Drive/SemesterProjectDatas/TACO/valid/images (300 images)\n",
            "  [4] /content/drive/My Drive/SemesterProjectDatas/ManualData/test/images (146 images)\n",
            "  [5] /content/drive/My Drive/SemesterProjectDatas/ManualData/train/images (3381 images)\n",
            "  [6] /content/drive/My Drive/SemesterProjectDatas/ManualData/valid/images (291 images)\n",
            "  [7] /content/drive/My Drive/SemesterProjectDatas/Model/Yolo12n/yolov12n_baseline (21 images)\n",
            "  [8] /content/drive/My Drive/SemesterProjectDatas/Model/Yolo12n/EnhancedYolo12n/run_yolo12n_enhanced_improved_stage1 (21 images)\n",
            "  [9] /content/drive/My Drive/SemesterProjectDatas/Model/Yolo12n/EnhancedYolo12n/run_yolo12n_enhanced_improved_stage2_qat (18 images)\n",
            "  [10] /content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12n/run_yolo12n_enhanced_qat_final (21 images)\n",
            "  [11] /content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nNew/run_yolo12n_enhanced_qat_final (2 images)\n",
            "  [12] /content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nNew/final_qat_yolo (2 images)\n",
            "  [13] /content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nModule/enhanced_yolo_no_quant (2 images)\n",
            "  [14] /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/enhanced_yolov12n (24 images)\n",
            "  [15] /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/Quantized (1 images)\n",
            "  [16] /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/Quantized/Visualizations (1 images)\n",
            "  [17] /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/Quantized/Evaluation (10 images)\n",
            "  [18] /content/drive/My Drive/SemesterProjectDatas/CombinedData/train/images (6527 images)\n",
            "  [19] /content/drive/My Drive/SemesterProjectDatas/CombinedData/valid/images (591 images)\n",
            "  [20] /content/drive/My Drive/SemesterProjectDatas/CombinedData/test/images (296 images)\n",
            "\n",
            "Enter the number of the directory to use for CALIBRATION:\n",
            "/content/drive/My Drive/SemesterProjectDatas/CombinedData/valid/images\n",
            "✓ Using first directory for calibration: /content/drive/My Drive/SemesterProjectDatas/ManualRecording\n",
            "\n",
            "Enter the number of the directory to use for EVALUATION (or press Enter to use the same as calibration):\n",
            "\n",
            "✓ Selected evaluation directory: /content/drive/My Drive/SemesterProjectDatas/ManualRecording\n",
            "Loaded data config with 11 classes\n",
            "Loading model from /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/enhanced_yolov12n/weights/best.pt\n",
            "✓ Model loaded successfully using ultralytics YOLO\n",
            "Enhancement modules detection:\n",
            "  - CBAM: Not found\n",
            "  - Transformer: Not found\n",
            "  - SmallObjectFeatures: Not found\n",
            "  - BiFPN: Not found\n",
            "WARNING: Some enhancement modules were not detected in the model.\n",
            "Exporting model to ONNX: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced.onnx\n",
            "✓ ONNX export completed successfully\n",
            "✓ ONNX model structure verified\n",
            "Verifying ONNX model...\n",
            "✓ ONNX model structure is valid\n",
            "✓ ONNX model verification passed - outputs match within tolerance\n",
            "Identifying attention-related nodes to protect during quantization...\n",
            "Found 182 attention-related nodes to protect\n",
            "\n",
            "Do you want to use calibration-based quantization or direct quantization?\n",
            "[1] Calibration-based (more accurate but requires valid images)\n",
            "[2] Direct quantization (faster, no calibration data needed)\n",
            "Enter your choice (1 or 2): 1\n",
            "Creating calibration data reader for: /content/drive/My Drive/SemesterProjectDatas/ManualRecording\n",
            "First few calibration images (up to 5):\n",
            "  1. /content/drive/My Drive/SemesterProjectDatas/ManualRecording/IMG_20250323_160539.jpg\n",
            "Found 1 calibration images in /content/drive/My Drive/SemesterProjectDatas/ManualRecording\n",
            "✓ Successfully loaded test image: shape=(3060, 4080, 3)\n",
            "Quantizing ONNX model to INT8: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\n",
            "✓ ONNX model loaded successfully. Model IR version: 7\n",
            "Model input name: input\n",
            "ONNX Runtime version: 1.22.0\n",
            "Starting quantization with CalibrationMethod.MinMax calibration method...\n",
            "Using quantization format: QOperator\n",
            "⚠ Primary quantization failed: quantize_static() got an unexpected keyword argument 'optimize_model'\n",
            "Trying alternative quantization method...\n",
            "Trying dynamic quantization without calibration...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Dynamic quantization complete (fallback method).\n",
            "⚠ Dynamically quantized model verification failed: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/model.0/conv/Conv_quant'\n",
            "Will fall back to model optimization without quantization...\n",
            "⚠ Dynamic quantization also failed: Model verification failed\n",
            "Trying ONNX Runtime optimization without quantization...\n",
            "✓ Model optimization completed successfully: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8_optimized.onnx\n",
            "✓ Optimized model copied to: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\n",
            "✓ Final model verified and saved to: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\n",
            "No valid images found for comparison\n",
            "\n",
            "Evaluating FP32 model:\n",
            "Evaluating model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced.onnx\n",
            "Found 1 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ Error processing /content/drive/My Drive/SemesterProjectDatas/ManualRecording/IMG_20250323_160539.jpg: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'resize'\n",
            "> Overload resolution failed:\n",
            ">  - Can't parse 'dsize'. Sequence item with index 0 has a wrong type\n",
            ">  - Can't parse 'dsize'. Sequence item with index 0 has a wrong type\n",
            "\n",
            "⚠ No valid images were processed during evaluation\n",
            "\n",
            "Evaluating INT8 model:\n",
            "Evaluating model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 test images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  7.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ Error processing /content/drive/My Drive/SemesterProjectDatas/ManualRecording/IMG_20250323_160539.jpg: OpenCV(4.11.0) :-1: error: (-5:Bad argument) in function 'resize'\n",
            "> Overload resolution failed:\n",
            ">  - Can't parse 'dsize'. Sequence item with index 0 has a wrong type\n",
            ">  - Can't parse 'dsize'. Sequence item with index 0 has a wrong type\n",
            "\n",
            "⚠ No valid images were processed during evaluation\n",
            "\n",
            "Could not calculate speedup due to missing metrics\n",
            "Metrics saved to /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/quantization_metrics.json\n",
            "\n",
            "Model export and quantization process complete!\n",
            "FP32 ONNX model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced.onnx\n",
            "INT8 ONNX model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied best INT8 model to: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/best_model_int8.onnx\n",
            "\n",
            "==================================================\n",
            "SUMMARY\n",
            "==================================================\n",
            "Original PyTorch model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/enhanced_yolov12n/weights/best.pt\n",
            "FP32 ONNX model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced.onnx\n",
            "INT8 ONNX model: /content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\n",
            "==================================================\n",
            "To use the INT8 model in your application, use the following:\n",
            "```python\n",
            "import onnxruntime as ort\n",
            "session = ort.InferenceSession(\"/content/drive/My Drive/SemesterProjectDatas/Model/NewEnhancedYolo12nModule/NewQuantized/yolov12n_enhanced_int8.onnx\")\n",
            "# Run inference with:\n",
            "# outputs = session.run(None, {\"input\": preprocessed_image})\n",
            "```\n",
            "==================================================\n",
            "✓ Successfully verified that the quantized model can be loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ltVTrk7SZu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}