{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# COMPLETE Enhanced YOLOv12n Training Script with Custom Enhancements, QAT,\n",
        "#  Results Processing\n",
        "################################################################################"
      ],
      "metadata": {
        "id": "uVd9jyX4_bHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Environment Setup\n",
        "# --------------------\n",
        "print(\"Phase 1: Environment Setup\")\n",
        "print(\"==========================\")\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Google Drive mount not required or failed:\", e)\n",
        "\n",
        "import subprocess\n",
        "print(\"Installing/Updating Ultralytics and other necessary packages...\")\n",
        "try:\n",
        "    # Ensure ultralytics is up-to-date for latest QAT features if any\n",
        "    subprocess.run([\"pip\", \"install\", \"--upgrade\", \"ultralytics\", \"torchvision\", \"torchaudio\"], check=True)\n",
        "    subprocess.run([\"pip\", \"install\", \"roboflow\", \"tqdm\", \"pandas\", \"seaborn\", \"matplotlib\", \"pyyaml\", \"scipy\"], check=True) # Added scipy for k-means\n",
        "    print(\"Packages installed/updated successfully.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error installing packages: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKS4CBIa_tDo",
        "outputId": "29d5efb7-e1ea-4fc5-986a-bed746ee2f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phase 1: Environment Setup\n",
            "==========================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "Installing/Updating Ultralytics and other necessary packages...\n",
            "Packages installed/updated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio ultralytics tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9v-4xlIIaB7",
        "outputId": "bd4fd5f3-225b-4eba-896d-4d1705a739e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.135)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "from scipy.cluster.vq import kmeans\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ultralytics import YOLO\n",
        "from torch.ao.quantization import QuantStub, DeQuantStub\n",
        "from ultralytics.nn.modules import Conv, C2f, Detect, Concat # Import building blocks\n",
        "from tqdm import tqdm as tqdm_iterator # Renamed to avoid conflict with YOLO's internal tqdm\n",
        "from IPython.display import Image, display # For displaying plots in Colab\n",
        "from ultralytics.utils.plotting import plot_results as ultralytics_plot_results # Alias for clarity\n",
        "from torch.ao.quantization import QuantStub, DeQuantStub\n",
        "\n",
        "\n",
        "print(\"All initial imports and setup completed.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgHkMKcA_5Ae",
        "outputId": "67aeef30-ea87-4dd7-8aa2-a9e71b99be7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All initial imports and setup completed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO12n model\n",
        "model = YOLO('yolo12n.pt')\n",
        "\n",
        "# Create dummy input tensor\n",
        "dummy_input = torch.randn(1, 3, 640, 640)\n",
        "\n",
        "# Extract backbone layers (all but the last Detect layer)\n",
        "layers = model.model.model[:-1]\n",
        "\n",
        "# Forward pass through the backbone, correctly handling Concat layers\n",
        "outputs = []\n",
        "x = dummy_input\n",
        "for layer in layers:\n",
        "    if layer.f != -1:\n",
        "        x = layer([outputs[i] for i in layer.f])\n",
        "    else:\n",
        "        x = layer(x)\n",
        "    outputs.append(x)\n",
        "\n",
        "# Last three outputs are backbone outputs (feature maps)\n",
        "p3, p4, p5 = outputs[-3], outputs[-2], outputs[-1]\n",
        "\n",
        "print(\"âœ… Backbone Feature Maps:\")\n",
        "print(f\"P3 shape: {p3.shape}\")\n",
        "print(f\"P4 shape: {p4.shape}\")\n",
        "print(f\"P5 shape: {p5.shape}\")\n",
        "\n",
        "# Inspect Detection Head configuration robustly\n",
        "detect_layer = model.model.model[-1]\n",
        "\n",
        "print(\"\\nðŸ”Ž Detection Head Configuration:\")\n",
        "anchors = getattr(detect_layer, 'anchors', None)\n",
        "print(\"Anchors:\", anchors if anchors is not None else \"Anchor-free\")\n",
        "\n",
        "strides = getattr(detect_layer, 'stride', None)\n",
        "print(\"Strides:\", strides.tolist() if strides is not None else \"Not found\")\n",
        "\n",
        "nl = getattr(detect_layer, 'nl', len(strides) if strides is not None else \"Unknown\")\n",
        "print(\"Number of detection layers (nl):\", nl)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7C5p3mOGRpc",
        "outputId": "e7d11cfb-7993-48e2-fc2c-ae6fba0f0233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Backbone Feature Maps:\n",
            "P3 shape: torch.Size([1, 128, 20, 20])\n",
            "P4 shape: torch.Size([1, 384, 20, 20])\n",
            "P5 shape: torch.Size([1, 256, 20, 20])\n",
            "\n",
            "ðŸ”Ž Detection Head Configuration:\n",
            "Anchors: tensor([[ 0.5000,  1.5000,  2.5000,  ...,  8.5000,  9.5000, 10.5000],\n",
            "        [ 0.5000,  0.5000,  0.5000,  ..., 20.5000, 20.5000, 20.5000]])\n",
            "Strides: [8.0, 16.0, 32.0]\n",
            "Number of detection layers (nl): 3\n",
            "YOLO(\n",
            "  (model): DetectionModel(\n",
            "    (model): Sequential(\n",
            "      (0): Conv(\n",
            "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Conv(\n",
            "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (2): C3k2(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(8, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (4): C3k2(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): Bottleneck(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): Conv(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (6): A2C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-1): 2 x Sequential(\n",
            "            (0): ABlock(\n",
            "              (attn): AAttn(\n",
            "                (qkv): Conv(\n",
            "                  (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (proj): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (pe): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "              (mlp): Sequential(\n",
            "                (0): Conv(\n",
            "                  (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (1): Conv(\n",
            "                  (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (1): ABlock(\n",
            "              (attn): AAttn(\n",
            "                (qkv): Conv(\n",
            "                  (conv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (proj): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (pe): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=64)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "              (mlp): Sequential(\n",
            "                (0): Conv(\n",
            "                  (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (1): Conv(\n",
            "                  (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (7): Conv(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (8): A2C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0-1): 2 x Sequential(\n",
            "            (0): ABlock(\n",
            "              (attn): AAttn(\n",
            "                (qkv): Conv(\n",
            "                  (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (proj): Conv(\n",
            "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (pe): Conv(\n",
            "                  (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "              (mlp): Sequential(\n",
            "                (0): Conv(\n",
            "                  (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (1): Conv(\n",
            "                  (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (1): ABlock(\n",
            "              (attn): AAttn(\n",
            "                (qkv): Conv(\n",
            "                  (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (proj): Conv(\n",
            "                  (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "                (pe): Conv(\n",
            "                  (conv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "              (mlp): Sequential(\n",
            "                (0): Conv(\n",
            "                  (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (1): Conv(\n",
            "                  (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): Identity()\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (9): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (10): Concat()\n",
            "      (11): A2C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): C3k(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv3): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (m): Sequential(\n",
            "              (0): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (1): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (12): Upsample(scale_factor=2.0, mode='nearest')\n",
            "      (13): Concat()\n",
            "      (14): A2C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): C3k(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv3): Conv(\n",
            "              (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (m): Sequential(\n",
            "              (0): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (1): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (15): Conv(\n",
            "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (16): Concat()\n",
            "      (17): A2C2f(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): C3k(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv3): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (m): Sequential(\n",
            "              (0): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (1): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (18): Conv(\n",
            "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "        (act): SiLU(inplace=True)\n",
            "      )\n",
            "      (19): Concat()\n",
            "      (20): C3k2(\n",
            "        (cv1): Conv(\n",
            "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (cv2): Conv(\n",
            "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "          (act): SiLU(inplace=True)\n",
            "        )\n",
            "        (m): ModuleList(\n",
            "          (0): C3k(\n",
            "            (cv1): Conv(\n",
            "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv2): Conv(\n",
            "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (cv3): Conv(\n",
            "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (m): Sequential(\n",
            "              (0): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (1): Bottleneck(\n",
            "                (cv1): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "                (cv2): Conv(\n",
            "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                  (act): SiLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (21): Detect(\n",
            "        (cv2): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Conv(\n",
            "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv(\n",
            "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "              (act): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (cv3): ModuleList(\n",
            "          (0): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): DWConv(\n",
            "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (1): Conv(\n",
            "                (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): DWConv(\n",
            "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (1): Conv(\n",
            "                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (1): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): DWConv(\n",
            "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (1): Conv(\n",
            "                (conv): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): DWConv(\n",
            "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (1): Conv(\n",
            "                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (2): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): DWConv(\n",
            "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (1): Conv(\n",
            "                (conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): DWConv(\n",
            "                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "              (1): Conv(\n",
            "                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
            "                (act): SiLU(inplace=True)\n",
            "              )\n",
            "            )\n",
            "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "        )\n",
            "        (dfl): DFL(\n",
            "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define Custom Enhancement Modules\n",
        "# ------------------------------------\n",
        "print(\"\\nPhase 2: Defining Custom Enhancement Modules\")\n",
        "print(\"==========================================\")\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=16):\n",
        "        super().__init__()\n",
        "        self.channel_mlp = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid_channel = nn.Sigmoid()\n",
        "        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
        "        self.sigmoid_spatial = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ca_weights = self.sigmoid_channel(self.channel_mlp(x))\n",
        "        x_ca = ca_weights * x\n",
        "        sa_input = torch.cat([torch.max(x_ca, dim=1, keepdim=True)[0], torch.mean(x_ca, dim=1, keepdim=True)], dim=1)\n",
        "        sa_weights = self.sigmoid_spatial(self.spatial_conv(sa_input))\n",
        "        x_sa = sa_weights * x_ca\n",
        "        return x_sa\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, channels, num_heads=4, num_layers=1, dim_feedforward=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if dim_feedforward is None:\n",
        "            dim_feedforward = channels * 2\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=channels,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,  # Critical for QAT compatibility\n",
        "            activation=F.relu\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Dequantize before Transformer\n",
        "        x = self.dequant(x)\n",
        "\n",
        "        b, c, h, w = x.size()\n",
        "        seq = x.view(b, c, -1).permute(0, 2, 1)  # Adjusted for batch_first=True\n",
        "        seq_out = self.transformer_encoder(seq)\n",
        "        x_out = seq_out.permute(0, 2, 1).view(b, c, h, w)\n",
        "\n",
        "        # Requantize after Transformer\n",
        "        x_out = self.quant(x_out)\n",
        "        return x_out\n",
        "\n",
        "class SmallObjectFeatures(nn.Module):\n",
        "    def __init__(self, in_channels, feature_channels=128):\n",
        "        super().__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            Conv(in_channels, feature_channels, k=3, p=1),\n",
        "            Conv(feature_channels, feature_channels, k=3, p=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv_block(x)\n",
        "\n",
        "class BiFPN_FusionNode(nn.Module):\n",
        "    def __init__(self, in_channels_high_res, in_channels_low_res, out_channels):\n",
        "        super().__init__()\n",
        "        # merge_conv after concatenation of two feature maps\n",
        "        self.merge_conv = Conv(in_channels_high_res + in_channels_low_res, out_channels, k=1, p=0)\n",
        "\n",
        "    def forward(self, x_high_res, x_low_res):\n",
        "        if x_low_res.shape[2:] != x_high_res.shape[2:]:\n",
        "            x_low_res = F.interpolate(x_low_res, size=x_high_res.shape[2:], mode='nearest')\n",
        "        fused = torch.cat([x_high_res, x_low_res], dim=1)\n",
        "        out = self.merge_conv(fused)\n",
        "        return out\n",
        "\n",
        "\n",
        "print(\"Custom enhancement modules defined.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAVRk6rI_7XG",
        "outputId": "c0edbb55-0f9f-42ba-9ae2-46412b79257a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phase 2: Defining Custom Enhancement Modules\n",
            "==========================================\n",
            "Custom enhancement modules defined.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define the Enhanced YOLO Model Structure\n",
        "# ---------------------------------------------\n",
        "print(\"\\nPhase 3: Defining Custom Enhanced YOLO Model Structure\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "class ModifiedYOLOBackbone(nn.Module):\n",
        "    def __init__(self, original_model_sequential):\n",
        "        super().__init__()\n",
        "        self.layers = list(original_model_sequential.children())\n",
        "        # Tap points deduced from yolo12n.pt structure (0-indexed)\n",
        "        self.p2_tap_idx = 2  # Output of layer 2 (original index) for P2 features\n",
        "        self.p3_tap_idx = 4  # Output of layer 4 for P3 features\n",
        "        self.p4_tap_idx = 6  # Output of layer 6 for P4 features\n",
        "        self.p5_tap_idx = 8  # Output of layer 8 for P5 features\n",
        "\n",
        "        self.stage1 = nn.Sequential(*self.layers[0 : self.p2_tap_idx + 1])\n",
        "        self.stage2 = nn.Sequential(*self.layers[self.p2_tap_idx + 1 : self.p3_tap_idx + 1])\n",
        "        self.stage3 = nn.Sequential(*self.layers[self.p3_tap_idx + 1 : self.p4_tap_idx + 1])\n",
        "        self.stage4 = nn.Sequential(*self.layers[self.p4_tap_idx + 1 : self.p5_tap_idx + 1])\n",
        "\n",
        "        # Critical fix for export compatibility\n",
        "        self.f = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        p2_feat = self.stage1(x)\n",
        "        p3_feat = self.stage2(p2_feat)\n",
        "        p4_feat = self.stage3(p3_feat)\n",
        "        p5_feat = self.stage4(p4_feat)\n",
        "        return p2_feat, p3_feat, p4_feat, p5_feat\n",
        "\n",
        "\n",
        "class CustomEnhancedYOLOModel(nn.Module):\n",
        "    def __init__(self, base_model_instance, nc):\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "\n",
        "        # âœ… Correct backbone channels as per provided YOLO12n backbone\n",
        "        self.ch_p3, self.ch_p4, self.ch_p5 = 128, 128, 256\n",
        "\n",
        "        # Unified BiFPN channel width\n",
        "        bifpn_channels = 256\n",
        "\n",
        "        # âœ… Modified YOLO backbone explicitly using provided backbone\n",
        "        self.modified_backbone = ModifiedYOLOBackbone(base_model_instance.model.model)\n",
        "\n",
        "        # âœ… Projection layers matching backbone outputs exactly\n",
        "        self.p3_proj = Conv(self.ch_p3, bifpn_channels, k=1)  # 128 -> 256\n",
        "        self.p4_proj = Conv(self.ch_p4, bifpn_channels, k=1)  # 384 -> 256\n",
        "        self.p5_proj = Conv(self.ch_p5, bifpn_channels, k=1)  # 256 -> 256\n",
        "\n",
        "        # âœ… Optional small-object features based on backbone output (64 channels at P2)\n",
        "        self.p2_feat_gen = SmallObjectFeatures(64, feature_channels=bifpn_channels)\n",
        "        self.p2_neck_conv = Conv(bifpn_channels, bifpn_channels, k=1)\n",
        "\n",
        "        # âœ… CBAM + Transformer layers match backbone exactly (384 input channels from P4)\n",
        "        self.cbam_p4 = CBAM(in_channels=self.ch_p4) # clearly 384 channels\n",
        "        self.transformer_p4 = TransformerEncoderBlock(self.ch_p4, num_heads=4)\n",
        "\n",
        "\n",
        "        # âœ… Correct BiFPN FusionNodes (input_channels = sum of two features after projection)\n",
        "        self.bifpn_p4_td = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "        self.bifpn_p3_td = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "        self.bifpn_p4_out = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "        self.bifpn_p5_out = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "\n",
        "        # âœ… Detection head explicitly expects BiFPN output channels\n",
        "        self.detect_head = Detect(nc=self.nc, ch=[bifpn_channels]*3)\n",
        "        self.detect_head.stride = torch.tensor([8.0, 16.0, 32.0])\n",
        "        self.stride = self.detect_head.stride\n",
        "\n",
        "        # âœ… Quantization stubs\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "        # âœ… Ultralytics compatibility\n",
        "        self.layers = nn.ModuleList([self.modified_backbone, self.detect_head])\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # Explicit backbone extraction\n",
        "        p2, p3, p4, p5 = self.modified_backbone(x)\n",
        "\n",
        "        print(\"P3 shape:\", p3.shape)\n",
        "        print(\"P4 shape:\", p4.shape)\n",
        "        print(\"P5 shape:\", p5.shape)\n",
        "\n",
        "        # Verify explicitly if needed\n",
        "        #assert p3.size(1) == 128 and p4.size(1) == 384 and p5.size(1) == 256, \"Backbone output mismatch!\"\n",
        "\n",
        "        # CBAM & Transformer explicitly applied BEFORE projection\n",
        "        p4_enh = self.cbam_p4(p4)\n",
        "        p4_enh = self.transformer_p4(p4_enh)\n",
        "\n",
        "        # Explicit projections\n",
        "        p3_proj = self.p3_proj(p3)\n",
        "        p4_proj = self.p4_proj(p4_enh)\n",
        "        p5_proj = self.p5_proj(p5)\n",
        "\n",
        "        # BiFPN explicitly\n",
        "        p4_td = self.bifpn_p4_td(p4_proj, p5_proj)\n",
        "        p3_td = self.bifpn_p3_td(p3_proj, p4_td)\n",
        "\n",
        "        p4_out = self.bifpn_p4_out(p4_td, p3_td)\n",
        "        p5_out = self.bifpn_p5_out(p5_proj, p4_out)\n",
        "\n",
        "        # Dequantize explicitly\n",
        "        features = [self.dequant(f) for f in [p3_td, p4_out, p5_out]]\n",
        "\n",
        "        detections = self.detect_head(features)\n",
        "\n",
        "        if self.training:\n",
        "            if targets is not None:\n",
        "                loss, loss_dict = self.compute_loss(detections, targets)\n",
        "                return loss, loss_dict\n",
        "            else:\n",
        "                return torch.zeros(1, device=x.device), {}\n",
        "        else:\n",
        "            return detections\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUJVCJ8j__wG",
        "outputId": "3ebade7b-5adc-41f2-ace6-7a8842f7f37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phase 3: Defining Custom Enhanced YOLO Model Structure\n",
            "======================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5a85Ow-ibCC",
        "outputId": "30fddf1d-534e-497d-8efe-f201bb5a0a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.22.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.135)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect import DetectionTrainer\n",
        "from ultralytics.utils.torch_utils import ModelEMA\n",
        "\n",
        "torch._dynamo.config.suppress_errors = True  # Suppress potential Dynamo-related warnings\n",
        "torch._dynamo.disable()\n",
        "\n",
        "# --- STEP 1: Define Paths ---\n",
        "drive_base_path = '/content/drive/My Drive/'\n",
        "dataset_root_in_drive = 'SemesterProjectDatas/CombinedData'\n",
        "project_root_in_drive = 'SemesterProjectDatas/Model/EnhancedYolo12nNew'\n",
        "\n",
        "combined_dataset_path = os.path.join(drive_base_path, dataset_root_in_drive)\n",
        "data_yaml = os.path.join(combined_dataset_path, 'data.yaml')\n",
        "base_model_name = 'yolo12n.pt'\n",
        "project_dir_enhanced_model = os.path.join(drive_base_path, project_root_in_drive)\n",
        "experiment_name = 'run_yolo12n_enhanced_qat_final'\n",
        "\n",
        "os.makedirs(project_dir_enhanced_model, exist_ok=True)\n",
        "\n",
        "with open(data_yaml, 'r') as f:\n",
        "    data_dict = yaml.safe_load(f)\n",
        "\n",
        "nc_from_yaml = data_dict['nc']\n",
        "class_names_from_yaml = data_dict['names']\n",
        "\n",
        "# --- STEP 2: Load base YOLO model explicitly ---\n",
        "base_yolo_for_weights = YOLO(base_model_name)\n",
        "\n",
        "# --- STEP 3: Instantiate Custom Enhanced YOLO Model ---\n",
        "custom_pytorch_model = CustomEnhancedYOLOModel(base_yolo_for_weights, nc=nc_from_yaml)\n",
        "custom_pytorch_model.load_state_dict(base_yolo_for_weights.model.state_dict(), strict=False)\n",
        "custom_pytorch_model.nc = nc_from_yaml\n",
        "custom_pytorch_model.names = dict(enumerate(class_names_from_yaml))\n",
        "custom_pytorch_model.yaml = base_yolo_for_weights.model.yaml\n",
        "custom_pytorch_model.yaml['nc'] = nc_from_yaml\n",
        "custom_pytorch_model.yaml['names'] = class_names_from_yaml\n",
        "\n",
        "# --- STEP 4: Freeze backbone explicitly ---\n",
        "for param in custom_pytorch_model.modified_backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# --- STEP 5: Forward patch for QAT ---\n",
        "original_forward = custom_pytorch_model.forward\n",
        "\n",
        "\n",
        "def qat_forward(x, targets=None):\n",
        "    if isinstance(x, dict):\n",
        "        for key in ['img', 'images', 'x']:\n",
        "            if key in x:\n",
        "                x = x[key]\n",
        "                break\n",
        "        else:\n",
        "            raise ValueError(\"Dictionary input missing image keys.\")\n",
        "\n",
        "    if custom_pytorch_model.training:\n",
        "        x = x.requires_grad_(True)\n",
        "\n",
        "    preds = original_forward(x, targets)\n",
        "\n",
        "    if custom_pytorch_model.training:\n",
        "        if isinstance(preds, tuple):\n",
        "            loss = preds[0]\n",
        "            if isinstance(loss, dict):\n",
        "                loss = sum(v for v in loss.values() if isinstance(v, torch.Tensor))\n",
        "            if not loss.requires_grad:\n",
        "                raise RuntimeError(\"Loss does not require grad; check forward implementation.\")\n",
        "            return loss\n",
        "        return preds if torch.is_tensor(preds) else preds[0]\n",
        "\n",
        "    return preds\n",
        "\n",
        "custom_pytorch_model.forward = qat_forward\n",
        "\n",
        "# --- STEP 6: QAT Preparation ---\n",
        "custom_pytorch_model.train()\n",
        "custom_pytorch_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "excluded_modules = (TransformerEncoderBlock, CBAM, BiFPN_FusionNode)\n",
        "for name, module in custom_pytorch_model.named_modules():\n",
        "    if isinstance(module, excluded_modules):\n",
        "        module.qconfig = None\n",
        "\n",
        "torch.quantization.prepare_qat(custom_pytorch_model, inplace=True)\n",
        "\n",
        "# --- STEP 7: Patch EMA for QAT compatibility ---\n",
        "def safe_ema_update(self, model):\n",
        "    with torch.no_grad():\n",
        "        msd = model.state_dict()\n",
        "        for k, ema_v in self.ema.state_dict().items():\n",
        "            if k in msd and ema_v.shape == msd[k].shape:\n",
        "                if ema_v.dtype.is_floating_point:\n",
        "                    ema_v.copy_(ema_v * self.decay + msd[k].detach() * (1 - self.decay))\n",
        "\n",
        "ModelEMA.update = safe_ema_update\n",
        "\n",
        "# --- STEP 8: Disable EMA completely in YOLO ---\n",
        "class CustomTrainer(DetectionTrainer):\n",
        "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
        "        model = custom_pytorch_model.to(self.device)\n",
        "        model.nc = self.data['nc']\n",
        "        model.names = self.data['names']\n",
        "        model.stride = base_yolo_for_weights.model.stride\n",
        "        return model\n",
        "\n",
        "    def build_ema(self, model):\n",
        "        return None\n",
        "\n",
        "# --- STEP 9: Assign model to YOLO explicitly ---\n",
        "managed_model = YOLO()\n",
        "managed_model.model = custom_pytorch_model\n",
        "managed_model.overrides['model'] = None\n",
        "\n",
        "# --- STEP 10: Training ---\n",
        "train_args = {\n",
        "    'data': data_yaml,\n",
        "    'epochs': 100,\n",
        "    'imgsz': 640,\n",
        "    'batch': 16,\n",
        "    'project': project_dir_enhanced_model,\n",
        "    'name': experiment_name,\n",
        "    'exist_ok': True,\n",
        "    'device': 'cuda',\n",
        "    'patience': 25,\n",
        "    'optimizer': 'AdamW',\n",
        "    'lr0': 0.001,\n",
        "    'lrf': 0.01,\n",
        "    'amp': False,\n",
        "    'trainer': CustomTrainer\n",
        "}\n",
        "\n",
        "results = managed_model.train(**train_args)\n",
        "\n",
        "# --- STEP 11: Save final model ---\n",
        "# Ensure the save directory exists\n",
        "model_save_dir = os.path.join(project_dir_enhanced_model, experiment_name)\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# Define the full path explicitly\n",
        "custom_model_save_path = os.path.join(model_save_dir, 'custom_enhanced_model.pth')\n",
        "\n",
        "# Save the model state dict\n",
        "torch.save(custom_pytorch_model.state_dict(), custom_model_save_path)\n",
        "\n",
        "print(f\"âœ… Custom Enhanced YOLOv12n QAT model saved at: {custom_model_save_path}\")\n",
        "\n",
        "# Save quantized model for deployment (optional)\n",
        "quantized_model_path = os.path.join(project_dir_enhanced_model, experiment_name, 'quantized_model.pt')\n",
        "# Convert QAT model to quantized model\n",
        "quantized_model = torch.quantization.convert(custom_pytorch_model.eval(), inplace=False)\n",
        "torch.save(quantized_model.state_dict(), quantized_model_path)\n",
        "print(f\"âœ… Quantized model saved at: {quantized_model_path}\")\n",
        "\n",
        "\n",
        "'''\n",
        "''''\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect import DetectionTrainer\n",
        "from ultralytics.utils.torch_utils import ModelEMA\n",
        "\n",
        "# Suppress Dynamo warnings\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.disable()\n",
        "\n",
        "# --- STEP 1: Define Paths ---\n",
        "drive_base_path = '/content/drive/My Drive/'\n",
        "dataset_root_in_drive = 'SemesterProjectDatas/CombinedData'\n",
        "project_root_in_drive = 'SemesterProjectDatas/Model/EnhancedYolo12nNew'\n",
        "\n",
        "combined_dataset_path = os.path.join(drive_base_path, dataset_root_in_drive)\n",
        "data_yaml = os.path.join(combined_dataset_path, 'data.yaml')\n",
        "base_model_name = 'yolo12n.pt'\n",
        "project_dir_enhanced_model = os.path.join(drive_base_path, project_root_in_drive)\n",
        "experiment_name = 'run_yolo12n_enhanced_qat_final'\n",
        "\n",
        "os.makedirs(project_dir_enhanced_model, exist_ok=True)\n",
        "\n",
        "with open(data_yaml, 'r') as f:\n",
        "    data_dict = yaml.safe_load(f)\n",
        "\n",
        "nc_from_yaml = data_dict['nc']\n",
        "class_names_from_yaml = data_dict['names']\n",
        "\n",
        "# --- STEP 2: Load base YOLO model explicitly ---\n",
        "base_yolo_for_weights = YOLO(base_model_name)\n",
        "\n",
        "# --- STEP 3: Instantiate Custom Enhanced YOLO Model ---\n",
        "custom_pytorch_model = CustomEnhancedYOLOModel(base_yolo_for_weights, nc=nc_from_yaml)\n",
        "custom_pytorch_model.load_state_dict(base_yolo_for_weights.model.state_dict(), strict=False)\n",
        "custom_pytorch_model.nc = nc_from_yaml\n",
        "custom_pytorch_model.names = dict(enumerate(class_names_from_yaml))\n",
        "custom_pytorch_model.yaml = base_yolo_for_weights.model.yaml\n",
        "custom_pytorch_model.yaml['nc'] = nc_from_yaml\n",
        "custom_pytorch_model.yaml['names'] = class_names_from_yaml\n",
        "\n",
        "# --- STEP 4: Freeze backbone explicitly ---\n",
        "for param in custom_pytorch_model.modified_backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# --- STEP 5: Forward patch for QAT with detailed debug statements ---\n",
        "original_forward = custom_pytorch_model.forward\n",
        "\n",
        "def qat_forward(x, targets=None):\n",
        "    if isinstance(x, dict):\n",
        "        for key in ['img', 'images', 'x']:\n",
        "            if key in x:\n",
        "                x = x[key]\n",
        "                break\n",
        "        else:\n",
        "            raise ValueError(\"Dictionary input missing image keys.\")\n",
        "\n",
        "    if custom_pytorch_model.training:\n",
        "        x = x.requires_grad_(True)\n",
        "\n",
        "    preds = original_forward(x, targets)\n",
        "    print(\"preds returned from original_forward:\", preds)\n",
        "\n",
        "    if custom_pytorch_model.training:\n",
        "        if isinstance(preds, tuple):\n",
        "            loss = preds[0]\n",
        "            print(\"Initial loss from tuple:\", loss)\n",
        "            if isinstance(loss, dict):\n",
        "                loss = sum(v for v in loss.values() if isinstance(v, torch.Tensor))\n",
        "                print(\"Summed loss from dict:\", loss)\n",
        "\n",
        "            print(\"loss.requires_grad:\", loss.requires_grad)\n",
        "            if not loss.requires_grad:\n",
        "                print(\"Problematic loss:\", loss)\n",
        "                raise RuntimeError(\"Loss does not require grad; check forward implementation.\")\n",
        "            return loss\n",
        "\n",
        "        print(\"Pred tensor requires_grad:\", preds.requires_grad if torch.is_tensor(preds) else preds[0].requires_grad)\n",
        "        return preds if torch.is_tensor(preds) else preds[0]\n",
        "\n",
        "    return preds\n",
        "\n",
        "custom_pytorch_model.forward = qat_forward\n",
        "\n",
        "# --- STEP 6: QAT Preparation ---\n",
        "custom_pytorch_model.train()\n",
        "custom_pytorch_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "excluded_modules = (TransformerEncoderBlock, CBAM, BiFPN_FusionNode)\n",
        "for name, module in custom_pytorch_model.named_modules():\n",
        "    if isinstance(module, excluded_modules):\n",
        "        module.qconfig = None\n",
        "\n",
        "torch.quantization.prepare_qat(custom_pytorch_model, inplace=True)\n",
        "\n",
        "# --- STEP 7: Patch EMA for QAT compatibility ---\n",
        "def safe_ema_update(self, model):\n",
        "    with torch.no_grad():\n",
        "        msd = model.state_dict()\n",
        "        for k, ema_v in self.ema.state_dict().items():\n",
        "            if k in msd and ema_v.shape == msd[k].shape:\n",
        "                if ema_v.dtype.is_floating_point:\n",
        "                    ema_v.copy_(ema_v * self.decay + msd[k].detach() * (1 - self.decay))\n",
        "\n",
        "ModelEMA.update = safe_ema_update\n",
        "\n",
        "# --- STEP 8: Disable EMA completely in YOLO ---\n",
        "class CustomTrainer(DetectionTrainer):\n",
        "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
        "        model = custom_pytorch_model.to(self.device)\n",
        "        model.nc = self.data['nc']\n",
        "        model.names = self.data['names']\n",
        "        model.stride = base_yolo_for_weights.model.stride\n",
        "        return model\n",
        "\n",
        "    def build_ema(self, model):\n",
        "        return None\n",
        "\n",
        "# --- STEP 9: Assign model to YOLO explicitly ---\n",
        "managed_model = YOLO()\n",
        "managed_model.model = custom_pytorch_model\n",
        "managed_model.overrides['model'] = None\n",
        "\n",
        "# --- STEP 10: Training ---\n",
        "train_args = {\n",
        "    'data': data_yaml,\n",
        "    'epochs': 100,\n",
        "    'imgsz': 640,\n",
        "    'batch': 16,\n",
        "    'project': project_dir_enhanced_model,\n",
        "    'name': experiment_name,\n",
        "    'exist_ok': True,\n",
        "    'device': 'cuda',\n",
        "    'patience': 25,\n",
        "    'optimizer': 'AdamW',\n",
        "    'lr0': 0.001,\n",
        "    'lrf': 0.01,\n",
        "    'amp': False,\n",
        "    'trainer': CustomTrainer\n",
        "}\n",
        "\n",
        "custom_pytorch_model.train()\n",
        "results = managed_model.train(**train_args)\n",
        "\n",
        "# --- STEP 11: Save final model ---\n",
        "model_save_dir = os.path.join(project_dir_enhanced_model, experiment_name)\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "custom_model_save_path = os.path.join(model_save_dir, 'custom_enhanced_model.pth')\n",
        "torch.save(custom_pytorch_model.state_dict(), custom_model_save_path)\n",
        "print(f\"âœ… Custom Enhanced YOLOv12n QAT model saved at: {custom_model_save_path}\")\n",
        "'''\n",
        "\n",
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from ultralytics.models.yolo.detect import DetectionTrainer\n",
        "from ultralytics.nn.modules import Conv, Detect\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "\n",
        "\n",
        "# Suppress Dynamo warnings\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.disable()\n",
        "\n",
        "\n",
        "# 1. Paths Setup\n",
        "base_path = '/content/drive/My Drive/'\n",
        "data_path = os.path.join(base_path, 'SemesterProjectDatas/CombinedData')\n",
        "model_path = os.path.join(base_path, 'SemesterProjectDatas/Model/EnhancedYolo12nNew')\n",
        "data_yaml = os.path.join(data_path, 'data.yaml')\n",
        "base_model_name = 'yolo12n.pt'\n",
        "\n",
        "with open(data_yaml, 'r') as f:\n",
        "    data_dict = yaml.safe_load(f)\n",
        "\n",
        "nc = data_dict['nc']\n",
        "\n",
        "# 2. Load YOLO base\n",
        "base_yolo = YOLO(base_model_name)\n",
        "\n",
        "\n",
        "\n",
        "class CustomEnhancedYOLOModel(nn.Module):\n",
        "    def __init__(self, base_model, nc):\n",
        "        super().__init__()\n",
        "        self.nc = nc\n",
        "        bifpn_channels = 256\n",
        "\n",
        "        self.modified_backbone = ModifiedYOLOBackbone(base_model.model.model)\n",
        "\n",
        "        self.p3_proj = Conv(128, bifpn_channels, k=1)\n",
        "        self.p4_proj = Conv(128, bifpn_channels, k=1)\n",
        "        self.p5_proj = Conv(256, bifpn_channels, k=1)\n",
        "\n",
        "        self.cbam_p4 = CBAM(128)\n",
        "        self.transformer_p4 = TransformerEncoderBlock(128)\n",
        "\n",
        "        self.bifpn_p4_td = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "        self.bifpn_p3_td = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "        self.bifpn_p4_out = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "        self.bifpn_p5_out = BiFPN_FusionNode(bifpn_channels, bifpn_channels, bifpn_channels)\n",
        "\n",
        "        self.detect_head = Detect(nc=self.nc, ch=[bifpn_channels]*3)\n",
        "        self.detect_head.stride = torch.tensor([8.0, 16.0, 32.0])\n",
        "\n",
        "        self.stride = self.detect_head.stride\n",
        "\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "\n",
        "        self.yaml = base_model.yaml\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        p2, p3, p4, p5 = self.modified_backbone(x)\n",
        "\n",
        "        p4_enh = self.cbam_p4(p4)\n",
        "        p4_enh = self.transformer_p4(p4_enh)\n",
        "\n",
        "        p3_proj, p4_proj, p5_proj = self.p3_proj(p3), self.p4_proj(p4_enh), self.p5_proj(p5)\n",
        "\n",
        "        p4_td = self.bifpn_p4_td(p4_proj, p5_proj)\n",
        "        p3_td = self.bifpn_p3_td(p3_proj, p4_td)\n",
        "        p4_out = self.bifpn_p4_out(p4_td, p3_td)\n",
        "        p5_out = self.bifpn_p5_out(p5_proj, p4_out)\n",
        "\n",
        "        feats = [self.dequant(f) for f in [p3_td, p4_out, p5_out]]\n",
        "        return self.detect_head(feats)\n",
        "\n",
        "# Instantiate custom model\n",
        "custom_model = CustomEnhancedYOLOModel(base_yolo, nc).cuda()\n",
        "\n",
        "# Freeze backbone\n",
        "for param in custom_model.modified_backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 4. Proper QAT Setup\n",
        "custom_model.train()\n",
        "custom_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "for module in custom_model.modules():\n",
        "    if isinstance(module, (TransformerEncoderBlock, CBAM, BiFPN_FusionNode)):\n",
        "        module.qconfig = None\n",
        "\n",
        "custom_model = torch.quantization.prepare_qat(custom_model, inplace=False).cuda()\n",
        "\n",
        "# 5. Trainer definition\n",
        "class QATTrainer(DetectionTrainer):\n",
        "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
        "        return custom_model\n",
        "\n",
        "    def build_ema(self, model):\n",
        "        return None\n",
        "\n",
        "# 6. Start Training\n",
        "train_args = {\n",
        "    'data': data_yaml,\n",
        "    'epochs': 100,\n",
        "    'imgsz': 640,\n",
        "    'batch': 16,\n",
        "    'project': model_path,\n",
        "    'name': 'final_qat_yolo',\n",
        "    'exist_ok': True,\n",
        "    'device': 'cuda',\n",
        "    'optimizer': 'AdamW',\n",
        "    'lr0': 0.001,\n",
        "    'lrf': 0.01,\n",
        "    'amp': False,\n",
        "    'patience': 25,\n",
        "    'trainer': QATTrainer\n",
        "}\n",
        "\n",
        "managed_model = YOLO()\n",
        "managed_model.model = custom_model\n",
        "managed_model.train(**train_args)\n",
        "\n",
        "# 7. Convert & Save\n",
        "custom_model.cpu().eval()\n",
        "quantized_model = torch.quantization.convert(custom_model.eval(), inplace=False)\n",
        "\n",
        "save_path = os.path.join(model_path, 'final_qat_yolo', 'quantized_model.pth')\n",
        "torch.save(quantized_model.state_dict(), save_path)\n",
        "\n",
        "print(f\"âœ… Quantized model saved at: {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C3Z3ScC8E1J3",
        "outputId": "146ac0d6-cdb1-410b-b3f2-9a791d17016e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.135 ðŸš€ Python-3.11.12 torch-2.7.0+cu118 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=False, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/My Drive/SemesterProjectDatas/CombinedData/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=final_qat_yolo, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=25, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nNew, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nNew/final_qat_yolo, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage1.2.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage2.1.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.0.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.0.1.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.0.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage3.1.m.1.1.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.0.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.0.1.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.0.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.qkv.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.qkv.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.qkv.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.proj.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.proj.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.proj.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.pe.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.pe.conv.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.pe.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.attn.pe.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.mlp.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.mlp.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.mlp.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.mlp.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.mlp.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.stage4.1.m.1.1.mlp.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv3.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv3.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.cv3.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.11.m.0.m.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv3.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv3.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.cv3.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.14.m.0.m.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.15.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.15.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.15.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv3.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv3.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.cv3.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.17.m.0.m.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.18.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.18.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.18.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv3.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv3.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.cv3.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.0.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.0.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.0.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.0.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.0.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.0.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.1.cv1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.1.cv1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.1.cv1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.1.cv2.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.1.cv2.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.20.m.0.m.1.cv2.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.0.2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.1.2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv2.2.2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.0.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.0.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.0.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.0.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.0.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.0.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.1.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.1.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.1.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.1.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.1.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.1.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.0.2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.0.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.0.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.0.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.0.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.0.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.0.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.1.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.1.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.1.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.1.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.1.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.1.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.1.2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.0.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.0.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.0.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.0.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.0.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.0.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.1.0.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.1.0.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.1.0.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.1.1.conv.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.1.1.bn.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.1.1.bn.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.2.weight'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "WARNING âš ï¸ setting 'requires_grad=True' for frozen layer 'modified_backbone.f.21.cv3.2.2.bias'. See ultralytics.engine.trainer for customization of frozen layers.\n",
            "Freezing layer 'modified_backbone.f.21.dfl.conv.weight'\n",
            "Freezing layer 'detect_head.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.4Â±0.1 ms, read: 29.9Â±18.7 MB/s, size: 63.4 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/SemesterProjectDatas/CombinedData/train/labels.cache... 6527 images, 1141 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6527/6527 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 9723, len(boxes) = 30435. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.8Â±0.3 ms, read: 16.8Â±6.9 MB/s, size: 108.3 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/SemesterProjectDatas/CombinedData/valid/labels.cache... 591 images, 96 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591/591 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 937, len(boxes) = 2808. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to /content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nNew/final_qat_yolo/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 140 weight(decay=0.0), 159 weight(decay=0.0005), 164 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/My Drive/SemesterProjectDatas/Model/EnhancedYolo12nNew/final_qat_yolo\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/408 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "fused_moving_avg_obs_fake_quant(): argument 'input' (position 1) must be Tensor, not dict",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6829a35925bd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0mmanaged_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0mmanaged_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m \u001b[0mmanaged_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;31m# 7. Convert & Save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6829a35925bd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodified_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1858\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1816\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1818\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantize.py\u001b[0m in \u001b[0;36m_observer_forward_hook\u001b[0;34m(self, input, output)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_observer_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;34mr\"\"\"Forward hook that calls observer on the output\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fake_quantize.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         return torch.fused_moving_avg_obs_fake_quant(\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserver_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: fused_moving_avg_obs_fake_quant(): argument 'input' (position 1) must be Tensor, not dict"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- STEP 12: Post-training quantization and export ---\n",
        "# Ensure training_successful is defined explicitly\n",
        "training_successful = os.path.exists(os.path.join(project_dir_enhanced_model, experiment_name, 'weights', 'best.pt'))\n",
        "\n",
        "if training_successful:\n",
        "    best_model_path = os.path.join(project_dir_enhanced_model, experiment_name, 'weights', 'best.pt')\n",
        "    print(f\"âœ… Found best model from training at: {best_model_path}\")\n",
        "\n",
        "    if os.path.exists(best_model_path):\n",
        "        # Load the trained YOLO model weights explicitly and correctly\n",
        "        loaded_weights = torch.load(best_model_path, map_location='cpu')\n",
        "\n",
        "        if 'model' in loaded_weights:\n",
        "            model_state_dict = loaded_weights['model'].state_dict()\n",
        "        elif 'state_dict' in loaded_weights:\n",
        "            model_state_dict = loaded_weights['state_dict']\n",
        "        else:\n",
        "            model_state_dict = loaded_weights\n",
        "\n",
        "        # Instantiate a fresh custom model for post-training quantization\n",
        "        # (separate from the QAT model we've been working with)\n",
        "        quant_base_model = YOLO(base_model_name)\n",
        "        ptq_model = CustomEnhancedYOLOModel(\n",
        "            base_model_instance=quant_base_model,\n",
        "            nc=nc_from_yaml\n",
        "        )\n",
        "        ptq_model.load_state_dict(model_state_dict, strict=False)\n",
        "        ptq_model.cpu().eval()\n",
        "\n",
        "        # Configure post-training quantization explicitly\n",
        "        ptq_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "        # Explicitly exclude incompatible modules from quantization\n",
        "        for name, module in ptq_model.named_modules():\n",
        "            if isinstance(module, (TransformerEncoderBlock, CBAM, BiFPN_FusionNode)):\n",
        "                module.qconfig = None\n",
        "                print(f\"ðŸš© Excluded '{name}' ({module.__class__.__name__}) from PTQ quantization.\")\n",
        "\n",
        "        # Prepare and convert to INT8 explicitly (post-training quantization)\n",
        "        torch.quantization.prepare(ptq_model, inplace=True)\n",
        "        ptq_quantized_model = torch.quantization.convert(ptq_model, inplace=True)\n",
        "\n",
        "        # Save INT8 post-training quantized PyTorch model explicitly\n",
        "        ptq_model_path = os.path.join(project_dir_enhanced_model, experiment_name, 'ptq_quantized_model_int8.pth')\n",
        "        os.makedirs(os.path.dirname(ptq_model_path), exist_ok=True)\n",
        "        torch.save(ptq_quantized_model.state_dict(), ptq_model_path)\n",
        "        print(f\"âœ… INT8 post-training quantized PyTorch model saved at: {ptq_model_path}\")\n",
        "\n",
        "        # Export the INT8 model to TFLite using YOLO's export functionality clearly\n",
        "        # We'll export both the QAT model and the PTQ model for comparison\n",
        "\n",
        "        # 1. Export QAT model to TFLite\n",
        "        print(\"\\n--- Exporting QAT model to TFLite ---\")\n",
        "        qat_export_model = YOLO(base_model_name)\n",
        "        qat_export_model.model.load_state_dict(quantized_model.state_dict(), strict=False)\n",
        "\n",
        "        # Explicitly perform export\n",
        "        qat_export_model.export(\n",
        "            format='tflite',\n",
        "            imgsz=640,  # explicitly match training image size\n",
        "            int8=True,\n",
        "            data=data_yaml,\n",
        "            device='cpu'\n",
        "        )\n",
        "\n",
        "        # Explicitly handle QAT TFLite paths\n",
        "        qat_default_tflite_path = os.path.join(qat_export_model.export_dir, 'model.tflite')\n",
        "        qat_target_tflite_path = os.path.join(project_dir_enhanced_model, experiment_name, 'model_qat_int8.tflite')\n",
        "\n",
        "        # Ensure directories exist explicitly\n",
        "        os.makedirs(os.path.dirname(qat_target_tflite_path), exist_ok=True)\n",
        "\n",
        "        # Move TFLite model explicitly after verifying existence\n",
        "        if os.path.exists(qat_default_tflite_path):\n",
        "            import shutil\n",
        "            shutil.copy2(qat_default_tflite_path, qat_target_tflite_path)  # Copy instead of move\n",
        "            print(f\"âœ… TFLite QAT-INT8 model exported successfully: {qat_target_tflite_path}\")\n",
        "        else:\n",
        "            print(f\"âŒ QAT TFLite export failed; file not found at: {qat_default_tflite_path}\")\n",
        "\n",
        "        # 2. Export PTQ model to TFLite\n",
        "        print(\"\\n--- Exporting PTQ model to TFLite ---\")\n",
        "        ptq_export_model = YOLO(base_model_name)\n",
        "        ptq_export_model.model.load_state_dict(ptq_quantized_model.state_dict(), strict=False)\n",
        "\n",
        "        # Explicitly perform export\n",
        "        ptq_export_model.export(\n",
        "            format='tflite',\n",
        "            imgsz=640,  # explicitly match training image size\n",
        "            int8=True,\n",
        "            data=data_yaml,\n",
        "            device='cpu'\n",
        "        )\n",
        "\n",
        "        # Explicitly handle PTQ TFLite paths\n",
        "        ptq_default_tflite_path = os.path.join(ptq_export_model.export_dir, 'model.tflite')\n",
        "        ptq_target_tflite_path = os.path.join(project_dir_enhanced_model, experiment_name, 'model_ptq_int8.tflite')\n",
        "\n",
        "        # Move TFLite model explicitly after verifying existence\n",
        "        if os.path.exists(ptq_default_tflite_path):\n",
        "            import shutil\n",
        "            shutil.copy2(ptq_default_tflite_path, ptq_target_tflite_path)  # Copy instead of move\n",
        "            print(f\"âœ… TFLite PTQ-INT8 model exported successfully: {ptq_target_tflite_path}\")\n",
        "        else:\n",
        "            print(f\"âŒ PTQ TFLite export failed; file not found at: {ptq_default_tflite_path}\")\n",
        "\n",
        "        print(\"\\n--- Quantization and Export Summary ---\")\n",
        "        print(\"1. QAT model: Trained with quantization awareness throughout training\")\n",
        "        print(\"2. PTQ model: Best model quantized after training\")\n",
        "        print(\"Compare these models to see which approach gives better results for your use case.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âŒ Best model weights not found at: {best_model_path}\")\n",
        "else:\n",
        "    print(\"\\nâŒ Training was unsuccessful or incomplete. Post-training INT8 conversion skipped.\")\n",
        "    print(\"However, the QAT model has still been saved and can be used for inference.\")\n"
      ],
      "metadata": {
        "id": "omtAJynCe96e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display, Image as IPImage\n",
        "from ultralytics.utils.plotting import plot_results as ultralytics_plot_results\n",
        "\n",
        "print(\"\\n\\nPhase 6: Post-Training Metrics, Plotting, and Export\")\n",
        "print(\"========================================================\")\n",
        "\n",
        "# Check if training was successful by looking for best.pt\n",
        "best_weights_path = os.path.join(project_dir_enhanced_model, experiment_name, 'weights', 'best.pt')\n",
        "training_successful = os.path.exists(best_weights_path)\n",
        "\n",
        "if training_successful:\n",
        "    current_run_dir = os.path.join(project_dir_enhanced_model, experiment_name)\n",
        "    path_to_best_model_pt = os.path.join(current_run_dir, 'weights', 'best.pt')\n",
        "\n",
        "    if os.path.exists(path_to_best_model_pt):\n",
        "        print(f\"Best QAT-trained model found: {path_to_best_model_pt}\")\n",
        "        eval_model = YOLO(path_to_best_model_pt) # Load the best QAT model for evaluation\n",
        "\n",
        "        print(\"\\nEvaluating QAT-trained model on validation set...\")\n",
        "        # Pass split='val' if your data.yaml has a 'val' key pointing to val images\n",
        "        # Ensure batch size for validation is manageable\n",
        "        val_metrics = eval_model.val(data=data_yaml, imgsz=image_size_for_training, batch=max(1, batch_size // 2), device=device_to_use, plots=True, split='val')\n",
        "\n",
        "        print(\"\\n--- Processing and Saving Metrics (as per user request) ---\")\n",
        "        # Convert numpy types to regular Python types (using val_metrics object)\n",
        "        metrics_dict = {\n",
        "            'mAP50-95': float(val_metrics.box.map) if hasattr(val_metrics.box, 'map') else None,\n",
        "            'mAP50': float(val_metrics.box.map50) if hasattr(val_metrics.box, 'map50') else None,\n",
        "            'mAP75': float(val_metrics.box.map75) if hasattr(val_metrics.box, 'map75') else None,\n",
        "            'Mean_Precision': float(val_metrics.box.mp) if hasattr(val_metrics.box, 'mp') else None,\n",
        "            'Mean_Recall': float(val_metrics.box.mr) if hasattr(val_metrics.box, 'mr') else None,\n",
        "            'Per_class_AP': val_metrics.box.maps.tolist() if hasattr(val_metrics.box, 'maps') and val_metrics.box.maps is not None else None,\n",
        "            'Per_class_precision': val_metrics.box.p.tolist() if hasattr(val_metrics.box, 'p') and val_metrics.box.p is not None else None,\n",
        "            'Per_class_recall': val_metrics.box.r.tolist() if hasattr(val_metrics.box, 'r') and val_metrics.box.r is not None else None,\n",
        "            'F1_scores': val_metrics.box.f1.tolist() if hasattr(val_metrics.box, 'f1') and val_metrics.box.f1 is not None else None,\n",
        "        }\n",
        "        if hasattr(val_metrics, 'speed') and isinstance(val_metrics.speed, dict):\n",
        "            metrics_dict['Inference_speed(ms)'] = {\n",
        "                'preprocessing': float(val_metrics.speed.get('preprocess', 0)),\n",
        "                'inference': float(val_metrics.speed.get('inference', 0)),\n",
        "                'postprocessing': float(val_metrics.speed.get('postprocess', 0))\n",
        "            }\n",
        "        elif hasattr(val_metrics, 'speed'): # if speed is a single value\n",
        "             metrics_dict['Inference_speed(ms)'] = float(val_metrics.speed)\n",
        "        else:\n",
        "            metrics_dict['Inference_speed(ms)'] = None\n",
        "\n",
        "        # Save metrics to JSON in the specific run directory\n",
        "        metrics_json_path = os.path.join(current_run_dir, 'evaluation_metrics.json')\n",
        "        with open(metrics_json_path, 'w') as f:\n",
        "            json.dump(metrics_dict, f, indent=4)\n",
        "        print(f\"Detailed evaluation metrics saved to: {metrics_json_path}\")\n",
        "\n",
        "        # Plot YOLO results (standard Ultralytics plots)\n",
        "        results_csv_path = os.path.join(current_run_dir, 'results.csv')\n",
        "        print(f\"Using results CSV for plotting: {results_csv_path}\")\n",
        "\n",
        "        if os.path.exists(results_csv_path):\n",
        "            try:\n",
        "                # This function directly shows plots if in interactive env.\n",
        "                ultralytics_plot_results(file=results_csv_path)\n",
        "                # Find the generated 'results.png' and save it with a more specific name if needed,\n",
        "                # or assume it's saved in current_run_dir by plot_results.\n",
        "                # Let's save the current figure if plot_results generates one.\n",
        "                plt.savefig(os.path.join(current_run_dir, 'training_progress_curves.png'))\n",
        "                # plt.show() # May not be needed if plot_results shows it.\n",
        "                print(f\"Ultralytics plot_results saved to {os.path.join(current_run_dir, 'training_progress_curves.png')}\")\n",
        "            except Exception as e_plot:\n",
        "                print(f\"Ultralytics plot_results failed: {e_plot}\")\n",
        "\n",
        "            # Add custom loss plots from CSV (User's requested format)\n",
        "            df = pd.read_csv(results_csv_path)\n",
        "            df.columns = df.columns.str.strip() # Clean column names\n",
        "\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "            plot_specs = [\n",
        "                ('train/box_loss', 'Box Loss', 'blue', (0,0)),\n",
        "                ('train/cls_loss', 'Classification Loss', 'orange', (0,1)),\n",
        "                ('train/dfl_loss', 'DFL Loss', 'green', (1,0))\n",
        "            ]\n",
        "            for col, title, color, pos in plot_specs:\n",
        "                if col in df.columns:\n",
        "                    axes[pos].plot(df['epoch'], df[col], label=title, color=color)\n",
        "                    axes[pos].set_title(f'Training {title}')\n",
        "                    axes[pos].set_xlabel('Epoch'); axes[pos].set_ylabel('Loss')\n",
        "                    axes[pos].legend(); axes[pos].grid(True)\n",
        "                else:\n",
        "                    print(f\"Column {col} not found for plotting.\")\n",
        "                    fig.delaxes(axes[pos]) # Remove unused subplot axis\n",
        "\n",
        "            # Validation mAP plot\n",
        "            ax_map = axes[1,1]\n",
        "            plotted_map = False\n",
        "            if 'metrics/mAP50(B)' in df.columns:\n",
        "                ax_map.plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP50 (Best)', color='red')\n",
        "                plotted_map = True\n",
        "            if 'metrics/mAP50-95(B)' in df.columns:\n",
        "                ax_map.plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP50-95 (Best)', color='purple')\n",
        "                plotted_map = True\n",
        "\n",
        "            if plotted_map:\n",
        "                ax_map.set_title('Validation mAP')\n",
        "                ax_map.set_xlabel('Epoch'); ax_map.set_ylabel('mAP')\n",
        "                ax_map.legend(); ax_map.grid(True)\n",
        "            else:\n",
        "                print(\"mAP columns not found for plotting.\")\n",
        "                fig.delaxes(ax_map)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            custom_loss_plot_path = os.path.join(current_run_dir, 'custom_loss_and_map_plots.png')\n",
        "            plt.savefig(custom_loss_plot_path)\n",
        "            plt.show()\n",
        "            print(f\"Custom loss and mAP plots saved to: {custom_loss_plot_path}\")\n",
        "\n",
        "            # Combined Training Losses Plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            plotted_any_combined_loss = False\n",
        "            if 'train/box_loss' in df.columns:\n",
        "                plt.plot(df['epoch'], df['train/box_loss'], label='Box Loss', linewidth=2)\n",
        "                plotted_any_combined_loss = True\n",
        "            if 'train/cls_loss' in df.columns:\n",
        "                plt.plot(df['epoch'], df['train/cls_loss'], label='Classification Loss', linewidth=2)\n",
        "                plotted_any_combined_loss = True\n",
        "            if 'train/dfl_loss' in df.columns:\n",
        "                plt.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss', linewidth=2)\n",
        "                plotted_any_combined_loss = True\n",
        "\n",
        "            if plotted_any_combined_loss:\n",
        "                plt.title('Training Losses over Epochs', fontsize=16)\n",
        "                plt.xlabel('Epoch', fontsize=14); plt.ylabel('Loss', fontsize=14)\n",
        "                plt.legend(fontsize=12); plt.grid(True, alpha=0.3)\n",
        "                combined_losses_path = os.path.join(current_run_dir, 'combined_training_losses.png')\n",
        "                plt.savefig(combined_losses_path, dpi=300)\n",
        "                plt.show()\n",
        "                print(f\"Combined training losses plot saved to: {combined_losses_path}\")\n",
        "            else:\n",
        "                print(\"No training loss columns found for combined plot.\")\n",
        "        else:\n",
        "            print(f\"Results CSV not found at {results_csv_path}, skipping custom plots.\")\n",
        "\n",
        "        # Display YOLO-generated plots (these are typically saved in the run directory)\n",
        "        print(\"\\n--- Displaying YOLO-generated Plots (if any) ---\")\n",
        "        yolo_plots_to_display = ['results.png', 'confusion_matrix.png', 'labels.jpg', 'PR_curve.png', 'F1_curve.png']\n",
        "        # Also add the custom plots we saved\n",
        "        custom_saved_plots = ['training_progress_curves.png', 'custom_loss_and_map_plots.png', 'combined_training_losses.png']\n",
        "\n",
        "        for plot_name in yolo_plots_to_display + custom_saved_plots:\n",
        "            plot_path = os.path.join(current_run_dir, plot_name)\n",
        "            if os.path.exists(plot_path):\n",
        "                print(f\"\\nDisplaying {plot_name}:\")\n",
        "                try:\n",
        "                    display(IPImage(filename=plot_path))\n",
        "                except Exception as img_e:\n",
        "                    print(f\"Could not display image {plot_path}: {img_e}\")\n",
        "            else:\n",
        "                print(f\"Plot {plot_name} not found in {current_run_dir}\")\n",
        "\n",
        "        # Export to TFLite INT8\n",
        "        try:\n",
        "            print(\"\\nExporting QAT-trained model to TFLite INT8...\")\n",
        "            tflite_path = eval_model.export(format='tflite', imgsz=image_size_for_training, int8=True, data=data_yaml, nms=True) # Add nms=True for better deployment\n",
        "            print(f\"TFLite INT8 model exported to: {tflite_path}\")\n",
        "        except Exception as e_export:\n",
        "            print(f\"Error exporting to TFLite INT8: {e_export}\")\n",
        "            print(\"Ensure the export format and parameters are compatible with your trained model and Ultralytics version.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Best QAT model (best.pt) not found at {path_to_best_model_pt}. Skipping final evaluation, plotting, and export.\")\n",
        "else:\n",
        "    print(\"\\nTraining was not successful or was interrupted. Skipping post-training steps.\")"
      ],
      "metadata": {
        "id": "GqDhk7Wa2Vcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "print(\"\\n\\nPhase 7: Evaluation and Metrics using INT8 Quantized Model\")\n",
        "print(\"===========================================================\")\n",
        "\n",
        "if training_successful:\n",
        "    current_run_dir = os.path.join(project_dir_enhanced_model, experiment_name)\n",
        "\n",
        "    # Explicitly Load your INT8-converted model instead of 'best.pt'\n",
        "    quantized_model_path = os.path.join(current_run_dir, 'quantized_model.pt')\n",
        "\n",
        "    if os.path.exists(quantized_model_path):\n",
        "        print(f\"INT8 Quantized model found: {quantized_model_path}\")\n",
        "\n",
        "        # âœ… Instantiate your custom model architecture explicitly\n",
        "        int8_model = CustomEnhancedYOLOModel(\n",
        "            base_model_instance=YOLO(base_model_name),  # Re-init base model clearly\n",
        "            nc=nc_from_yaml\n",
        "        )\n",
        "\n",
        "        # âœ… Prepare and convert explicitly for QAT (INT8)\n",
        "        int8_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
        "        torch.quantization.prepare_qat(int8_model, inplace=True)\n",
        "        int8_model = torch.quantization.convert(int8_model, inplace=True)\n",
        "\n",
        "        # âœ… Load INT8 weights explicitly\n",
        "        int8_model.load_state_dict(torch.load(quantized_model_path))\n",
        "        int8_model.eval()\n",
        "\n",
        "        # âœ… Wrap the INT8 model explicitly into YOLO container\n",
        "        eval_model = YOLO(base_model_name)  # fresh YOLO instance\n",
        "        eval_model.model = int8_model\n",
        "        eval_model.model.names = dict(enumerate(class_names_from_yaml))\n",
        "        eval_model.model.nc = nc_from_yaml\n",
        "        eval_model.model.stride = int8_model.stride\n",
        "        eval_model.model.yaml = custom_pytorch_model.yaml  # Copy YAML config for compatibility\n",
        "\n",
        "        print(\"\\nEvaluating INT8 Quantized model on validation set...\")\n",
        "        val_metrics_int8 = eval_model.val(\n",
        "            data=data_yaml,\n",
        "            imgsz=image_size_for_training,\n",
        "            batch=max(1, batch_size // 2),\n",
        "            device='cpu',  # INT8 quantized model evaluation on CPU only\n",
        "            plots=True,\n",
        "            split='val'\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Saving INT8 Quantized Model Metrics ---\")\n",
        "\n",
        "        # âœ… INT8 Quantized Metrics clearly labeled\n",
        "        metrics_dict_int8 = {\n",
        "            'mAP50-95': float(val_metrics_int8.box.map),\n",
        "            'mAP50': float(val_metrics_int8.box.map50),\n",
        "            'mAP75': float(val_metrics_int8.box.map75),\n",
        "            'Mean_Precision': float(val_metrics_int8.box.mp),\n",
        "            'Mean_Recall': float(val_metrics_int8.box.mr),\n",
        "            'Per_class_AP': val_metrics_int8.box.maps.tolist(),\n",
        "            'Per_class_precision': val_metrics_int8.box.p.tolist(),\n",
        "            'Per_class_recall': val_metrics_int8.box.r.tolist(),\n",
        "            'F1_scores': val_metrics_int8.box.f1.tolist(),\n",
        "        }\n",
        "\n",
        "        if hasattr(val_metrics_int8, 'speed') and isinstance(val_metrics_int8.speed, dict):\n",
        "            metrics_dict_int8['Inference_speed(ms)'] = {\n",
        "                'preprocessing': float(val_metrics_int8.speed.get('preprocess', 0)),\n",
        "                'inference': float(val_metrics_int8.speed.get('inference', 0)),\n",
        "                'postprocessing': float(val_metrics_int8.speed.get('postprocess', 0))\n",
        "            }\n",
        "        elif hasattr(val_metrics_int8, 'speed'):\n",
        "            metrics_dict_int8['Inference_speed(ms)'] = float(val_metrics_int8.speed)\n",
        "        else:\n",
        "            metrics_dict_int8['Inference_speed(ms)'] = None\n",
        "\n",
        "        # âœ… Save metrics to JSON clearly labeled for INT8 model\n",
        "        metrics_int8_json_path = os.path.join(current_run_dir, 'evaluation_metrics_INT8.json')\n",
        "        with open(metrics_int8_json_path, 'w') as f:\n",
        "            json.dump(metrics_dict_int8, f, indent=4)\n",
        "\n",
        "        print(f\"INT8 evaluation metrics saved to: {metrics_int8_json_path}\")\n",
        "\n",
        "        # âœ… Export INT8 quantized model explicitly to TFLite (Recommended)\n",
        "        try:\n",
        "            print(\"\\nExporting INT8 Quantized model to TFLite INT8 format...\")\n",
        "            tflite_int8_path = eval_model.export(\n",
        "                format='tflite', imgsz=image_size_for_training,\n",
        "                int8=True, data=data_yaml, nms=True, device='cpu'\n",
        "            )\n",
        "            print(f\"âœ… TFLite INT8 model exported to: {tflite_int8_path}\")\n",
        "\n",
        "            # Move to a standard location\n",
        "            import shutil\n",
        "            tflite_target_path = os.path.join(current_run_dir, 'model_int8.tflite')\n",
        "            if os.path.exists(tflite_int8_path):\n",
        "                shutil.copy2(tflite_int8_path, tflite_target_path)\n",
        "                print(f\"âœ… TFLite INT8 model copied to: {tflite_target_path}\")\n",
        "        except Exception as e_export:\n",
        "            print(f\"âŒ Error exporting to TFLite INT8: {e_export}\")\n",
        "            print(\"Ensure compatibility between INT8 model and Ultralytics export methods.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"âŒ INT8 Quantized model not found at {quantized_model_path}. Skipping evaluation.\")\n",
        "else:\n",
        "    print(\"\\nâŒ Training failed or was interrupted. Skipping INT8 evaluation and plots.\")\n",
        "\n",
        "print(\"\\nâœ… Phase 7 execution completed.\")"
      ],
      "metadata": {
        "id": "eNp0D1_H2ZFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_from_drive(\n",
        "    model_path,\n",
        "    input_video_path,\n",
        "    output_video_path=None,\n",
        "    conf_threshold=0.25,\n",
        "    fps_target=1\n",
        "):\n",
        "    \"\"\"\n",
        "    Process a video file from Google Drive using the quantized model,\n",
        "    save the detection results as a new video.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the quantized model (.pth or .tflite)\n",
        "        input_video_path: Path to the input video file in Google Drive\n",
        "        output_video_path: Path to save the output video (if None, will generate automatically)\n",
        "        conf_threshold: Confidence threshold for detections (0-1)\n",
        "        fps_target: Target frames per second for processing (default: 1)\n",
        "\n",
        "    Returns:\n",
        "        output_path: Path to the saved output video\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    import time\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Validate input video path\n",
        "    if not os.path.exists(input_video_path):\n",
        "        raise FileNotFoundError(f\"Input video not found at: {input_video_path}\")\n",
        "\n",
        "    # Generate output path if not provided\n",
        "    if output_video_path is None:\n",
        "        video_name = os.path.basename(input_video_path)\n",
        "        video_name_no_ext = os.path.splitext(video_name)[0]\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_video_path = os.path.join(\n",
        "            os.path.dirname(input_video_path),\n",
        "            f\"{video_name_no_ext}_detected_{timestamp}.mp4\"\n",
        "        )\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
        "\n",
        "    print(f\"Loading model"
      ],
      "metadata": {
        "id": "HNiDLlSK2cLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}